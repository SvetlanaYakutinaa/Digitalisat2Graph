{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelltraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datenkonvertierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"training_data_fixed_.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    training_data = [json.loads(line) for line in f if line.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Kann Span nicht bilden",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m     span = doc.char_span(start, end, label=label, alignment_mode=\u001b[33m\"\u001b[39m\u001b[33mcontract\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m span \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     17\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mKann Span nicht bilden\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m     ents.append(span)\n\u001b[32m     21\u001b[39m doc.ents = ents\n",
      "\u001b[31mValueError\u001b[39m: Kann Span nicht bilden"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank(\"de\")\n",
    "\n",
    "db = DocBin()\n",
    "\n",
    "for item in training_data:\n",
    "    text, annotations = item[0], item[1]\n",
    "\n",
    "    doc = nlp.make_doc(text)\n",
    "\n",
    "    ents = []\n",
    "    for start, end, label in annotations:\n",
    "\n",
    "        span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "        if span is None:\n",
    "            \n",
    "            raise ValueError(\n",
    "                f\"Kann Span nicht bilden\")\n",
    "        ents.append(span)\n",
    "\n",
    "\n",
    "    doc.ents = ents\n",
    "    db.add(doc)\n",
    "\n",
    "db.to_disk(\"output/train.spacy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mar\n",
      "[('Mar. Dor. Seidelin', 33, 38, 'PER'), ('Mar.', 33, 35, 'DATE')]\n"
     ]
    }
   ],
   "source": [
    "# Welches Token ist \"4\"?\n",
    "print(doc[33].text)\n",
    "\n",
    "# Zeig alle Spans, die Token 4 enthalten\n",
    "conflicts = [(s.text, s.start, s.end, s.label_) for s in ents if s.start <= 33 < s.end]\n",
    "print(conflicts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.util import filter_spans\n",
    "\n",
    "nlp = spacy.blank(\"de\")\n",
    "db = DocBin()\n",
    "\n",
    "def make_spans(doc, annotations, ex_id=\"\"):\n",
    "    made = []\n",
    "    for (start, end, label) in annotations:\n",
    "        # 1) Basis-Checks\n",
    "        if not (0 <= start < end <= len(doc.text)):\n",
    "            raise ValueError(f\"[{ex_id}] Ungültige Offsets: {(start, end, label)} | len(text)={len(doc.text)}\")\n",
    "\n",
    "        # 2) Erst 'contract', dann Fallback 'expand'\n",
    "        span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "        if span is None:\n",
    "            span = doc.char_span(start, end, label=label, alignment_mode=\"expand\")\n",
    "\n",
    "        # 3) Wenn immer noch None: präzise Fehlermeldung\n",
    "        if span is None:\n",
    "            snippet = doc.text[max(0, start-25):min(len(doc.text), end+25)].replace(\"\\n\", \" \")\n",
    "            frag = doc.text[start:end]\n",
    "            raise ValueError(\n",
    "                f\"[{ex_id}] Kann Span nicht bilden für {frag!r} [{start},{end}) {label}. \"\n",
    "                f\"Kontext: …{snippet}…\"\n",
    "            )\n",
    "        made.append(span)\n",
    "\n",
    "    # 4) Duplikate raus + Überlappungen filtern (behält längste Spans)\n",
    "    uniq = {(s.start, s.end, s.label_): s for s in made}.values()\n",
    "    cleaned = filter_spans(list(uniq))\n",
    "    return cleaned\n",
    "\n",
    "for i, (text, annotations) in enumerate(training_data):\n",
    "    doc = nlp.make_doc(text)\n",
    "    ents = make_spans(doc, annotations, ex_id=f\"ex#{i}\")\n",
    "    doc.ents = ents\n",
    "    db.add(doc)\n",
    "\n",
    "db.to_disk(\"output/train.spacy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[38;5;1m✘ The provided output file already exists. To force overwriting the\n",
      "config file, set the --force or -F flag.\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Trainiere das Modell mit spaCy\n",
    "!python3.11 -m spacy init config config.cfg --lang de --pipeline ner --optimize efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Saving to output directory: output\u001b[0m\n",
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[38;5;4mℹ To switch to GPU 0, use the option: --gpu-id 0\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
      "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  --------  ------  ------  ------  ------\n",
      "  0       0          0.00     55.11    0.00    0.00    0.00    0.00\n",
      "  0     200        113.66   2279.88   86.60   90.67   82.88    0.87\n",
      "  0     400        669.46    924.70   91.51   94.50   88.70    0.92\n",
      "  1     600        183.16    747.65   94.48   95.45   93.53    0.94\n",
      "  2     800        205.83    724.07   96.76   97.25   96.27    0.97\n",
      "  2    1000        223.96    594.70   98.15   98.31   97.99    0.98\n",
      "  4    1200        234.76    466.08   98.07   97.99   98.15    0.98\n",
      "  5    1400        443.32    379.55   98.55   98.50   98.61    0.99\n",
      "  6    1600        450.62    386.03   99.48   99.42   99.55    0.99\n",
      "  8    1800        517.74    307.79   99.69   99.75   99.64    1.00\n",
      " 11    2000        486.23    292.15   99.69   99.62   99.76    1.00\n",
      " 14    2200        554.82    226.78   99.90   99.93   99.87    1.00\n",
      " 17    2400        395.68    135.51   99.94   99.96   99.91    1.00\n",
      " 21    2600        482.15    139.07   99.95   99.91   99.98    1.00\n",
      " 25    2800        454.67    126.01   99.94   99.95   99.93    1.00\n",
      " 28    3000        696.93    146.82  100.00  100.00  100.00    1.00\n",
      " 32    3200        337.85     75.70   99.97   99.96   99.98    1.00\n",
      " 36    3400        625.82    110.73   99.98   99.98   99.98    1.00\n",
      " 39    3600        484.22     91.84   99.97   99.96   99.98    1.00\n",
      " 43    3800        420.19     69.18   99.98  100.00   99.96    1.00\n",
      " 46    4000        633.43     83.65  100.00  100.00  100.00    1.00\n",
      " 50    4200        651.71     90.33  100.00  100.00  100.00    1.00\n",
      " 54    4400       1511.87     91.21   99.95   99.93   99.96    1.00\n",
      " 57    4600       3230.89    206.89   99.91   99.91   99.91    1.00\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "output/model-last\n"
     ]
    }
   ],
   "source": [
    "!python3.11 -m spacy train config.cfg --output output --paths.train output/train.spacy --paths.dev output/train.spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelltest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('DATE', 'LOC', 'PER')\n"
     ]
    }
   ],
   "source": [
    "# spaCy-Modell importieren\n",
    "ner_model = spacy.load(\"output/model-best\")\n",
    "\n",
    "# Liste aller NER-Labels anzeigen\n",
    "labels = ner_model.get_pipe(\"ner\").labels\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test auf Textdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\" In meinem 14ten Jahr wurde ich von einem der obgedachten Prediger confirmirt.\n",
    "Im Jahr 1799 kam ich bei einem Strumpfwirker-Meister in die Lehre.\n",
    "Als ich im Jahr 1802 ausgelernt hatte, beschloss ich, sogleich auf die Wanderschaft zu gehen.\n",
    "In Heidelberg, wo ich nun wieder arbeitete.\n",
    "In Gnadau bekam ich sogleich Arbeit auf meiner Profession.\n",
    "Im März 1807 begab ich mich auf die Reise nach Herrnhut.\n",
    "Am 27sten September desselben Jahres wurde ich in den Brüderbund aufgenommen.\n",
    "Im Januar 1809 wurde mir angezeigt, dass ich Arbeit bekommen könnte.\n",
    "Am 6 April 1815 erging der Ruf des Herrn an mich, Ihm bei der Mission in Südafrika zu dienen.\n",
    "Am 25. Juli wurden wir in der Unitäts-Aeltesten-Konferenz abgefertigt.\n",
    "Wir traten am folgenden Tag die Reise an.\n",
    "Am 12. August langten wir in London an.\n",
    "Am 30. September verließen wir London.\n",
    "Am 24. December langten wir in der Capstadt an.\n",
    "Am Nachmittag des 30. Decembers erreichten wir Grönekloof.\n",
    "Nachdem wir mit der Hottentotten - Gemeine das Neujahrs- und Heidenfest 1816 gefeiert hatten, verließen wir Grönekloof.\n",
    "Langten nach einer fünftägigen Reise in Gnadenthal an, dem nunmehrigen Ort meiner Bestimmung.\n",
    "Am 3. März desselben Jahres wurde mir der Antrag gemacht, mit der ledigen Schwester Agnes Jenke in den Stand der heiligen Ehe zu treten.\n",
    "Am 26. März wurden wir getraut.\n",
    "Am 9. Februar 1817 wurden wir durch die Geburt eines Söhnleins erfreut.\n",
    "Langten wir am 8. Mai in Enon an.\n",
    "Am 20. Januar 1822 wurde mir in einer Versammlung des Hausgemeinleins eine vom Bischof Gottlob Martin Schneider ausgefertigte schriftliche Ordination zu einem Diakonus überreicht.\n",
    "Anfangs Februar 1825 verließen wir Kapstadt.\n",
    "Langten wir am 17. April in London an.\n",
    "Am 20. Mai in Kleinwelke eintrafen.\n",
    "Am 13. Juli traten wir die Rückreise nach Südafrika wieder an.\n",
    "Nach einem 14-tägigen Aufenthalt da selbst begaben wir uns über Neuwied und Zeist nach London und von da nach Bedford.\n",
    "Am 25. Februar 1826 langten wir nach 15 Wochen auf der stürmischen See in Kapstadt an.\n",
    "Wir reisten nun über Grönekloof nach Gnadenthal.\n",
    "Im Februar 1828 reisten wir zuvorderst nach der Kapstadt.\n",
    "Wir verließen im März 1829 Gnadenthal.\n",
    "Unser Weg führte uns zuerst nach Enon.\n",
    "Am 17. August 1830 verließen wir Silo.\n",
    "Nach einer zwölftägigen Reise in Enon an.\n",
    "Als wir im November 1839 in Enon anlangten, sah es da selbst gar traurig aus.\n",
    "Nachdem derselbe in Ebersdorf mit der ledigen Schwester Rosalie Bauer verbunden worden und diese unsere geliebten Kinder sich noch einige Zeit bei uns aufhielten, sah ich dieselben mit dankbar gebeugtem Herzen ihrem hohen Berufe entgegen gehen.\n",
    "Johannes Lemmerz, heimgegangen in Kleinwelke den 6. Mai 1855.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spaCy-Modell importieren\n",
    "ner_model = spacy.load(\"output/model-best\")\n",
    "\n",
    "doc = ner_model(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1799 DATE\n",
      "1802 DATE\n",
      "Heidelberg LOC\n",
      "Gnadau LOC\n",
      "März 1807 DATE\n",
      "Herrnhut LOC\n",
      "September DATE\n",
      "Januar 1809 DATE\n",
      "6 April 1815 DATE\n",
      "London LOC\n",
      "24. December DATE\n",
      "1816 DATE\n",
      "Gnadenthal LOC\n",
      "Agnes Jenke PER\n",
      "26. März DATE\n",
      "9. Februar 1817 DATE\n",
      "8. Mai DATE\n",
      "Enon LOC\n",
      "20. Januar 1822 DATE\n",
      "Gottlob Martin Schneider PER\n",
      "Februar 1825 DATE\n",
      "17. April DATE\n",
      "London LOC\n",
      "20. Mai DATE\n",
      "Kleinwelke LOC\n",
      "London LOC\n",
      "Bedford LOC\n",
      "25. Februar 1826 DATE\n",
      "Kapstadt LOC\n",
      "Gnadenthal LOC\n",
      "Februar 1828 DATE\n",
      "März 1829 DATE\n",
      "Gnadenthal LOC\n",
      "Enon LOC\n",
      "17. August 1830 DATE\n",
      "Enon LOC\n",
      "November 1839 DATE\n",
      "Enon LOC\n",
      "Ebersdorf LOC\n",
      "Rosalie Bauer PER\n",
      "Johannes Lemmerz PER\n",
      "Kleinwelke LOC\n",
      "6. Mai DATE\n"
     ]
    }
   ],
   "source": [
    "# Erkenne und gebe die Entitäten aus\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testdatenkonvertierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_bin = DocBin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_data.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        example = json.loads(line)\n",
    "        doc = nlp.make_doc(example[\"text\"])\n",
    "\n",
    "        valid_ents = []\n",
    "        occupied = set()\n",
    "\n",
    "        for start, end, label in example[\"entities\"]:\n",
    "            if any(i in occupied for i in range(start, end)):\n",
    "                continue  # überspringe überlappende Entitäten\n",
    "            span = doc.char_span(start, end, label=label)\n",
    "            if span:\n",
    "                valid_ents.append(span)\n",
    "                occupied.update(range(start, end))\n",
    "\n",
    "        doc.ents = valid_ents\n",
    "        doc_bin.add(doc)\n",
    "\n",
    "doc_bin.to_disk(\"output/test.spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testausführung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ner-Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[38;5;4mℹ To switch to GPU 0, use the option: --gpu-id 0\u001b[0m\n",
      "\u001b[1m\n",
      "================================== Results ==================================\u001b[0m\n",
      "\n",
      "TOK     100.00\n",
      "NER P   69.93 \n",
      "NER R   59.30 \n",
      "NER F   64.18 \n",
      "SPEED   26701 \n",
      "\n",
      "\u001b[1m\n",
      "=============================== NER (per type) ===============================\u001b[0m\n",
      "\n",
      "           P       R       F\n",
      "DATE   72.97   68.69   70.77\n",
      "LOC    78.00   75.92   76.95\n",
      "PER    47.99   34.79   40.34\n",
      "ORG    53.33   11.68   19.16\n",
      "\n",
      "\u001b[38;5;2m✔ Saved results to output/metrics.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3.11 -m spacy evaluate output/model-best output/test.spacy --output output/metrics.json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### spaCy de_core_news_lg Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[38;5;4mℹ To switch to GPU 0, use the option: --gpu-id 0\u001b[0m\n",
      "\u001b[1m\n",
      "================================== Results ==================================\u001b[0m\n",
      "\n",
      "TOK      100.00\n",
      "TAG      -     \n",
      "POS      -     \n",
      "MORPH    -     \n",
      "LEMMA    -     \n",
      "UAS      -     \n",
      "LAS      -     \n",
      "NER P    37.31 \n",
      "NER R    41.85 \n",
      "NER F    39.45 \n",
      "SENT P   -     \n",
      "SENT R   -     \n",
      "SENT F   -     \n",
      "SPEED    9871  \n",
      "\n",
      "\u001b[1m\n",
      "=============================== NER (per type) ===============================\u001b[0m\n",
      "\n",
      "           P       R       F\n",
      "LOC    61.33   79.03   69.06\n",
      "PER    21.87   35.79   27.15\n",
      "DATE    0.00    0.00    0.00\n",
      "MISC    0.00    0.00    0.00\n",
      "ORG    14.05   10.76   12.19\n",
      "\n",
      "\u001b[38;5;2m✔ Saved results to output/news_metrics.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3.11 -m spacy evaluate de_core_news_lg ./output/test.spacy --output output/news_metrics.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_bin = DocBin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_data.cleaned.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        example = json.loads(line)\n",
    "        doc = nlp.make_doc(example[\"text\"])\n",
    "\n",
    "        valid_ents = []\n",
    "        occupied = set()\n",
    "\n",
    "        for start, end, label in example[\"entities\"]:\n",
    "            if any(i in occupied for i in range(start, end)):\n",
    "                continue  # überspringe überlappende Entitäten\n",
    "            span = doc.char_span(start, end, label=label)\n",
    "            if span:\n",
    "                valid_ents.append(span)\n",
    "                occupied.update(range(start, end))\n",
    "\n",
    "        doc.ents = valid_ents\n",
    "        doc_bin.add(doc)\n",
    "\n",
    "doc_bin.to_disk(\"output/1_test.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[38;5;4mℹ To switch to GPU 0, use the option: --gpu-id 0\u001b[0m\n",
      "\u001b[1m\n",
      "================================== Results ==================================\u001b[0m\n",
      "\n",
      "TOK     100.00\n",
      "NER P   49.94 \n",
      "NER R   37.79 \n",
      "NER F   43.02 \n",
      "SPEED   27930 \n",
      "\n",
      "\u001b[1m\n",
      "=============================== NER (per type) ===============================\u001b[0m\n",
      "\n",
      "           P       R       F\n",
      "LOC    59.50   56.14   57.77\n",
      "DATE   66.67   43.14   52.39\n",
      "PER    13.75   10.93   12.18\n",
      "ORG     0.00    0.00    0.00\n",
      "\n",
      "\u001b[38;5;2m✔ Saved results to output/metrics.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3.11 -m spacy evaluate output/model-best output/1_test.spacy --output output/metrics.json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pfad zu deiner JSON-Datei\n",
    "with open(\"output/training_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    training_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"de\")  # leeres deutsches Modell\n",
    "doc_bin = DocBin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ungültiger Span in Satz: Am 15. Juni 1859 eröffnete Johann Philipp George auf dem Menger-Gelände sein neues Unternehmen und durchfahre die ersten Schritte in seinem neuen Lebensabschnitt.\n",
      "Ungültiger Span in Satz: Anna Charlotte Luther wackelt am 24. Mai 1841 auf dem Gnadenfreier Friedhof, während sie entdeckt, dass die Schönheit des Ortes ihre Seele berührt.\n",
      "Ungültiger Span in Satz: Georg Traneker bemüht sich dabei sein, sich anpassen zu können, um als neue Seele in der Herrnhuter Gemeinschaft Fuß fassen zu können, bevor er im September 1867 nach Herrnhut kommen möchte und seine Heimat verlassen will.\n",
      "Ungültiger Span in Satz: Am 15. Dezember 1883 begreift Johanne Gysbertine Eliſabeth Pronkert die Chancen des Ackerbodens für ihre neue landwirtschaftliche Initiative und vorantreibt ihre Pläne, um den Ertrag zu steigern.\n",
      "Ungültiger Span in Satz: Am 15. Mai 1832 erhielt Anna Marie Menze auf ihrer Reise nach Thüringen eine freundliche Begrüßung von den Bewohnern Neudietendorfs, die sie teilen wollten mit der jungen Frau, die sich anpassen musste an das neue Umfeld.\n",
      "Ungültiger Span in Satz: Johann Chriſtian Seifart analysiert im Januar 1849 die wirtschaftlichen Perspektiven Gnadenfreis, um sein Unternehmen zu gestalten.\n",
      "Ungültiger Span in Satz: Guſtav Traugott Schulze⸗Roͤchling untersucht die historischen Dokumente des Herrnhuter Brüderhofes, als er am 15. Januar 1883 ankommt.\n",
      "Ungültiger Span in Satz: Am 15. März 1864 überschreiten Kathrina Dorothea Ramſch mit ihrem Gatten die Stadtgrenzen Gnadenfreis, um sich auf ihre neue Heimat einzulassen.\n",
      "Ungültiger Span in Satz: Am 15. Oktober 1820 setzt Maria Joſepha Eliſabeth Steinauer ihre Bemühungen ein, um die Notlage der Herrnhuter Familien zu verbessern, die durch den verheerenden Hagelschlag betroffen sind.\n",
      "Ungültiger Span in Satz: Fanny Köhler vernehmen am 25. Mai 1825 die atemraubende Schönheit der Sarepta-Landschaft während ihres Umzugs dorthin.\n",
      "Ungültiger Span in Satz: Am 15. Mai 1874 drehen sich Auguſt Heinrich Franks Gedanken in Gnadenfrei um die Verwirklichung seines langgehegten Traums einer selbstständigen Tätigkeit.\n",
      "Ungültiger Span in Satz: Ernſt Siegmund Fockel verliert sich auf dem Marktplatz von Bedford in der Abenddämmerung des 14. Dezember 1887.\n",
      "Ungültiger Span in Satz: Wilhelm Häuſer verlassen Gnadenfrei am 10. September 1887.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "[E1010] Unable to set entity information for token 2 which is included in more than one span in entities, blocked, missing or outside.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[71]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m      9\u001b[39m     ents.append(span)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43mdoc\u001b[49m\u001b[43m.\u001b[49m\u001b[43ments\u001b[49m = ents\n\u001b[32m     11\u001b[39m doc_bin.add(doc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/spacy/tokens/doc.pyx:798\u001b[39m, in \u001b[36mspacy.tokens.doc.Doc.ents.__set__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/spacy/tokens/doc.pyx:835\u001b[39m, in \u001b[36mspacy.tokens.doc.Doc.set_ents\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: [E1010] Unable to set entity information for token 2 which is included in more than one span in entities, blocked, missing or outside."
     ]
    }
   ],
   "source": [
    "for text, annot in training_data:\n",
    "    doc = nlp.make_doc(text)\n",
    "    ents = []\n",
    "    for start, end, label in annot[\"entities\"]:\n",
    "        span = doc.char_span(start, end, label=label)\n",
    "        if span is None:\n",
    "            print(f\"Ungültiger Span in Satz: {text}\")\n",
    "            continue\n",
    "        ents.append(span)\n",
    "    doc.ents = ents\n",
    "    doc_bin.add(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Umwandlung abgeschlossen\n"
     ]
    }
   ],
   "source": [
    "# Export in .spacy-Format (spaCy Binary Format)\n",
    "doc_bin.to_disk(\"./train.spacy\")\n",
    "print(\"Umwandlung abgeschlossen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[38;5;1m✘ The provided output file already exists. To force overwriting the\n",
      "config file, set the --force or -F flag.\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Trainiere das Modell mit spaCy\n",
    "!python3.11 -m spacy init config config.cfg --lang de --pipeline ner --optimize efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Saving to output directory: output\u001b[0m\n",
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[38;5;4mℹ To switch to GPU 0, use the option: --gpu-id 0\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
      "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  --------  ------  ------  ------  ------\n",
      "  0       0          0.00     44.86    3.43    2.22    7.47    0.03\n",
      "  3     200        297.90   1845.12   96.03   96.03   96.03    0.96\n",
      "  7     400         66.48    145.07   99.92  100.00   99.84    1.00\n",
      " 13     600         22.98     12.83  100.00  100.00  100.00    1.00\n",
      " 20     800         16.99     10.81  100.00  100.00  100.00    1.00\n",
      " 28    1000          1.75      1.12   99.84   99.84   99.84    1.00\n",
      " 38    1200         47.25     21.26  100.00  100.00  100.00    1.00\n",
      " 51    1400         22.65      5.39  100.00  100.00  100.00    1.00\n",
      " 66    1600         80.81     17.22  100.00  100.00  100.00    1.00\n",
      " 85    1800         13.94      3.58  100.00  100.00  100.00    1.00\n",
      "109    2000         78.17     15.01  100.00  100.00  100.00    1.00\n",
      "136    2200         54.73     14.40  100.00  100.00  100.00    1.00\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "output/model-last\n"
     ]
    }
   ],
   "source": [
    "# Starte das Training des Modells\n",
    "!python3.11 -m spacy train config.cfg --output ./output --paths.train ./train.spacy --paths.dev ./train.spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modell erfolgreich geladen von output/model-best\n"
     ]
    }
   ],
   "source": [
    "# Überprüfe, ob das Modell erfolgreich gespeichert wurde und lade es\n",
    "try:\n",
    "    # Achte darauf, den richtigen Pfad zu verwenden\n",
    "    model_path = 'output/model-best'  # Pfad zum gespeicherten Modell\n",
    "    ner_model = spacy.load(model_path)\n",
    "    print(f\"Modell erfolgreich geladen von {model_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Fehler beim Laden des Modells: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\" In meinem 14ten Jahr wurde ich von einem der obgedachten Prediger confirmirt.\n",
    "Im Jahr 1799 kam ich bei einem Strumpfwirker-Meister in die Lehre.\n",
    "Als ich im Jahr 1802 ausgelernt hatte, beschloss ich, sogleich auf die Wanderschaft zu gehen.\n",
    "In Heidelberg, wo ich nun wieder arbeitete.\n",
    "In Gnadau bekam ich sogleich Arbeit auf meiner Profession.\n",
    "Im März 1807 begab ich mich auf die Reise nach Herrnhut.\n",
    "Am 27sten September desselben Jahres wurde ich in den Brüderbund aufgenommen.\n",
    "Im Januar 1809 wurde mir angezeigt, dass ich Arbeit bekommen könnte.\n",
    "Am 6. April 1815 erging der Ruf des Herrn an mich, Ihm bei der Mission in Südafrika zu dienen.\n",
    "Am 25. Juli wurden wir in der Unitäts-Aeltesten-Konferenz abgefertigt.\n",
    "Wir traten am folgenden Tag die Reise an.\n",
    "Am 12. August langten wir in London an.\n",
    "Am 30. September verließen wir London.\n",
    "Am 24. December langten wir in der Capstadt an.\n",
    "Am Nachmittag des 30. Decembers erreichten wir Grönekloof.\n",
    "Nachdem wir mit der Hottentotten - Gemeine das Neujahrs- und Heidenfest 1816 gefeiert hatten, verließen wir Grönekloof.\n",
    "Langten nach einer fünftägigen Reise in Gnadenthal an, dem nunmehrigen Ort meiner Bestimmung.\n",
    "Am 3. März desselben Jahres wurde mir der Antrag gemacht, mit der ledigen Schwester Agnes Jenke in den Stand der heiligen Ehe zu treten.\n",
    "Am 26. März wurden wir getraut.\n",
    "Am 9. Februar 1817 wurden wir durch die Geburt eines Söhnleins erfreut.\n",
    "Langten wir am 8. Mai in Enon an.\n",
    "Am 20. Januar 1822 wurde mir in einer Versammlung des Hausgemeinleins eine vom Bischof Gottlob Martin Schneider ausgefertigte schriftliche Ordination zu einem Diakonus überreicht.\n",
    "Anfangs Februar 1825 verließen wir Kapstadt.\n",
    "Langten wir am 17. April in London an.\n",
    "Am 20. Mai in Kleinwelke eintrafen.\n",
    "Am 13. Juli traten wir die Rückreise nach Südafrika wieder an.\n",
    "Nach einem 14-tägigen Aufenthalt da selbst begaben wir uns über Neuwied und Zeist nach London und von da nach Bedford.\n",
    "Am 25. Februar 1826 langten wir nach 15 Wochen auf der stürmischen See in Kapstadt an.\n",
    "Wir reisten nun über Grönekloof nach Gnadenthal.\n",
    "Im Februar 1828 reisten wir zuvorderst nach der Kapstadt.\n",
    "Wir verließen im März 1829 Gnadenthal.\n",
    "Unser Weg führte uns zuerst nach Enon.\n",
    "Am 17. August 1830 verließen wir Silo.\n",
    "Nach einer zwölftägigen Reise in Enon an.\n",
    "Als wir im November 1839 in Enon anlangten, sah es da selbst gar traurig aus.\n",
    "Nachdem derselbe in Ebersdorf mit der ledigen Schwester Rosalie Bauer verbunden worden und diese unsere geliebten Kinder sich noch einige Zeit bei uns aufhielten, sah ich dieselben mit dankbar gebeugtem Herzen ihrem hohen Berufe entgegen gehen.\n",
    "Johannes Lemmerz, heimgegangen in Kleinwelke den 6. Mai 1855.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = ner_model(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jahr 1799 DATE\n",
      "Jahr 1802 DATE\n",
      "Heidelberg LOC\n",
      "Gnadau LOC\n",
      "März 1807 DATE\n",
      "Herrnhut LOC\n",
      "Januar 1809 DATE\n",
      "April 1815 DATE\n",
      "Südafrika LOC\n",
      "Juli wurden DATE\n",
      "August langten DATE\n",
      "London LOC\n",
      "September verließen DATE\n",
      "December langten DATE\n",
      "Heidenfest 1816 DATE\n",
      "Gnadenthal LOC\n",
      "März desselben DATE\n",
      "Agnes Jenke PER\n",
      "März wurden DATE\n",
      "Februar 1817 DATE\n",
      "Enon LOC\n",
      "Januar 1822 DATE\n",
      "Bischof Gottlob Martin Schneider PER\n",
      "Anfangs Februar 1825 PER\n",
      "London LOC\n",
      "Mai in DATE\n",
      "Kleinwelke LOC\n",
      "Februar 1826 DATE\n",
      "Kapstadt LOC\n",
      "Februar 1828 DATE\n",
      "März 1829 DATE\n",
      "Gnadenthal LOC\n",
      "August 1830 DATE\n",
      "Enon LOC\n",
      "November 1839 DATE\n",
      "Enon LOC\n",
      "Ebersdorf LOC\n",
      "Rosalie Bauer PER\n",
      "Johannes Lemmerz PER\n",
      "Kleinwelke LOC\n",
      "Mai LOC\n"
     ]
    }
   ],
   "source": [
    "# Erkenne und gebe die Entitäten aus\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_acc: 1.0\n",
      "token_p: 1.0\n",
      "token_r: 1.0\n",
      "token_f: 1.0\n",
      "sents_p: None\n",
      "sents_r: None\n",
      "sents_f: None\n",
      "tag_acc: None\n",
      "pos_acc: None\n",
      "morph_acc: None\n",
      "morph_micro_p: None\n",
      "morph_micro_r: None\n",
      "morph_micro_f: None\n",
      "morph_per_feat: None\n",
      "dep_uas: None\n",
      "dep_las: None\n",
      "dep_las_per_type: None\n",
      "ents_p: 0.14814814814814814\n",
      "ents_r: 0.5\n",
      "ents_f: 0.22857142857142856\n",
      "ents_per_type: {'MISC': {'p': 0.0, 'r': 0.0, 'f': 0.0}, 'DATE': {'p': 0.0, 'r': 0.0, 'f': 0.0}, 'LOC': {'p': 0.23529411764705882, 'r': 1.0, 'f': 0.38095238095238093}, 'PER': {'p': 0.0, 'r': 0.0, 'f': 0.0}}\n",
      "cats_score: 0.0\n",
      "cats_score_desc: macro F\n",
      "cats_micro_p: 0.0\n",
      "cats_micro_r: 0.0\n",
      "cats_micro_f: 0.0\n",
      "cats_macro_p: 0.0\n",
      "cats_macro_r: 0.0\n",
      "cats_macro_f: 0.0\n",
      "cats_macro_auc: 0.0\n",
      "cats_f_per_type: {}\n",
      "cats_auc_per_type: {}\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"de_core_news_lg\")\n",
    "\n",
    "\n",
    "# Funktion zum Laden der Testdaten aus einer JSON-Datei\n",
    "def load_test_data(json_file_path):\n",
    "    with open(json_file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "        \n",
    "    test_data = []\n",
    "    for item in data:\n",
    "        text = item[0]  # Der Text ist das erste Element der Liste\n",
    "        entities = item[1].get(\"entities\", [])  # Entitäten befinden sich im zweiten Element\n",
    "        test_data.append([text, {\"entities\": entities}])\n",
    "\n",
    "    return test_data\n",
    "\n",
    "# Beispiel zum Laden der Testdaten\n",
    "json_file_path = 'test_data.json'  # Ersetze dies mit dem tatsächlichen Pfad zu deiner JSON-Datei\n",
    "\n",
    "# Lade die Testdaten aus der JSON-Datei\n",
    "TEST_DATA = load_test_data(json_file_path)\n",
    "\n",
    "# Modell evaluieren\n",
    "results = evaluate(nlp, TEST_DATA)\n",
    "\n",
    "# Ergebnisse ausdrucken\n",
    "for key, val in results.items():\n",
    "    print(f\"{key}: {val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_acc: 1.0\n",
      "token_p: 1.0\n",
      "token_r: 1.0\n",
      "token_f: 1.0\n",
      "sents_p: None\n",
      "sents_r: None\n",
      "sents_f: None\n",
      "tag_acc: None\n",
      "pos_acc: None\n",
      "morph_acc: None\n",
      "morph_micro_p: None\n",
      "morph_micro_r: None\n",
      "morph_micro_f: None\n",
      "morph_per_feat: None\n",
      "dep_uas: None\n",
      "dep_las: None\n",
      "dep_las_per_type: None\n",
      "ents_p: 0.25\n",
      "ents_r: 0.375\n",
      "ents_f: 0.3\n",
      "ents_per_type: {'DATE': {'p': 0.0, 'r': 0.0, 'f': 0.0}, 'LOC': {'p': 0.42857142857142855, 'r': 0.75, 'f': 0.5454545454545454}, 'PER': {'p': 0.0, 'r': 0.0, 'f': 0.0}}\n",
      "cats_score: 0.0\n",
      "cats_score_desc: macro F\n",
      "cats_micro_p: 0.0\n",
      "cats_micro_r: 0.0\n",
      "cats_micro_f: 0.0\n",
      "cats_macro_p: 0.0\n",
      "cats_macro_r: 0.0\n",
      "cats_macro_f: 0.0\n",
      "cats_macro_auc: 0.0\n",
      "cats_f_per_type: {}\n",
      "cats_auc_per_type: {}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Funktion zum Laden der Testdaten aus einer JSON-Datei\n",
    "def load_test_data(json_file_path):\n",
    "    with open(json_file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "        \n",
    "    test_data = []\n",
    "    for item in data:\n",
    "        text = item[0]  # Der Text ist das erste Element der Liste\n",
    "        entities = item[1].get(\"entities\", [])  # Entitäten befinden sich im zweiten Element\n",
    "        test_data.append([text, {\"entities\": entities}])\n",
    "\n",
    "    return test_data\n",
    "\n",
    "# Beispiel zum Laden der Testdaten\n",
    "json_file_path = 'test_data.json'  # Ersetze dies mit dem tatsächlichen Pfad zu deiner JSON-Datei\n",
    "\n",
    "# Lade die Testdaten aus der JSON-Datei\n",
    "TEST_DATA = load_test_data(json_file_path)\n",
    "\n",
    "# Modell evaluieren\n",
    "results = evaluate(ner_model, TEST_DATA)\n",
    "\n",
    "# Ergebnisse ausdrucken\n",
    "for key, val in results.items():\n",
    "    print(f\"{key}: {val}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_acc: 1.0\n",
      "token_p: 1.0\n",
      "token_r: 1.0\n",
      "token_f: 1.0\n",
      "tag_acc: None\n",
      "pos_acc: None\n",
      "morph_acc: None\n",
      "morph_micro_p: None\n",
      "morph_micro_r: None\n",
      "morph_micro_f: None\n",
      "morph_per_feat: None\n",
      "sents_p: None\n",
      "sents_r: None\n",
      "sents_f: None\n",
      "dep_uas: None\n",
      "dep_las: None\n",
      "dep_las_per_type: None\n",
      "lemma_acc: None\n",
      "ents_p: 0.14814814814814814\n",
      "ents_r: 0.5\n",
      "ents_f: 0.22857142857142856\n",
      "ents_per_type: {'MISC': {'p': 0.0, 'r': 0.0, 'f': 0.0}, 'DATE': {'p': 0.0, 'r': 0.0, 'f': 0.0}, 'LOC': {'p': 0.23529411764705882, 'r': 1.0, 'f': 0.38095238095238093}, 'PER': {'p': 0.0, 'r': 0.0, 'f': 0.0}}\n",
      "speed: 8762.35563752798\n"
     ]
    }
   ],
   "source": [
    "from spacy.training.example import Example\n",
    "\n",
    "# Modell laden\n",
    "nlp = spacy.load(\"de_core_news_lg\")\n",
    "\n",
    "# Funktion zum Laden der Testdaten als spaCy Examples\n",
    "def load_test_data_as_examples(json_file_path, nlp):\n",
    "    with open(json_file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    examples = []\n",
    "    for item in data:\n",
    "        text = item[0]\n",
    "        annotations = item[1]\n",
    "        \n",
    "        doc = nlp.make_doc(text)  # Nur Tokenisierung, keine Vorhersagen\n",
    "        example = Example.from_dict(doc, annotations)\n",
    "        examples.append(example)\n",
    "    \n",
    "    return examples\n",
    "\n",
    "# Lade Testdaten\n",
    "json_file_path = \"test_data.json\"\n",
    "examples = load_test_data_as_examples(json_file_path, nlp)\n",
    "\n",
    "# Modell evaluieren\n",
    "results = nlp.evaluate(examples)\n",
    "\n",
    "# Ergebnisse ausgeben\n",
    "for key, val in results.items():\n",
    "    print(f\"{key}: {val}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.training.example import Example\n",
    "from spacy.tokens import DocBin, Doc\n",
    "import json\n",
    "\n",
    "def load_test_data_as_examples(json_file_path, ner_model):\n",
    "    with open(json_file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    examples = []\n",
    "    for item in data:\n",
    "        text = item[0]\n",
    "        annotations = item[1]\n",
    "        doc = ner_model.make_doc(text)\n",
    "\n",
    "        ents = []\n",
    "        for start, end, label in annotations.get(\"entities\", []):\n",
    "            span = doc.char_span(start, end, label=label, alignment_mode=\"expand\")\n",
    "            if span is not None:\n",
    "                ents.append(span)\n",
    "            else:\n",
    "                print(f\"Warnung: Entität konnte nicht aligned werden: '{text[start:end]}'\")\n",
    "\n",
    "        doc.ents = ents\n",
    "        example = Example.from_dict(doc, {\"entities\": [(ent.start_char, ent.end_char, ent.label_) for ent in ents]})\n",
    "        examples.append(example)\n",
    "\n",
    "    return examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluationsergebnisse:\n",
      "token_acc: 1.0000\n",
      "token_p: 1.0000\n",
      "token_r: 1.0000\n",
      "token_f: 1.0000\n",
      "ents_p: 0.2500\n",
      "ents_r: 0.3750\n",
      "ents_f: 0.3000\n",
      "ents_per_type: {'DATE': {'p': 0.0, 'r': 0.0, 'f': 0.0}, 'LOC': {'p': 0.42857142857142855, 'r': 0.75, 'f': 0.5454545454545454}, 'PER': {'p': 0.0, 'r': 0.0, 'f': 0.0}}\n",
      "speed: 15638.0315\n"
     ]
    }
   ],
   "source": [
    "# Modell evaluieren\n",
    "results = ner_model.evaluate(examples)\n",
    "\n",
    "# Ergebnisse ausgeben\n",
    "print(\"\\nEvaluationsergebnisse:\")\n",
    "for key, val in results.items():\n",
    "    if isinstance(val, float):\n",
    "        print(f\"{key}: {val:.4f}\")\n",
    "    else:\n",
    "        print(f\"{key}: {val}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Ungültiger Span: (57, 63) → 'Menger'\n",
      "→ Ganzer Text: Am 15. Juni 1859 eröffnete Johann Philipp George auf dem Menger-Gelände sein neues Unternehmen und durchfahre die ersten Schritte in seinem neuen Lebensabschnitt.\n",
      "\n",
      "⚠️ Ungültiger Span: (54, 64) → 'Gnadenfrei'\n",
      "→ Ganzer Text: Anna Charlotte Luther wackelt am 24. Mai 1841 auf dem Gnadenfreier Friedhof, während sie entdeckt, dass die Schönheit des Ortes ihre Seele berührt.\n",
      "\n",
      "⚠️ Ungültiger Span: (89, 97) → 'Herrnhut'\n",
      "→ Ganzer Text: Georg Traneker bemüht sich dabei sein, sich anpassen zu können, um als neue Seele in der Herrnhuter Gemeinschaft Fuß fassen zu können, bevor er im September 1867 nach Herrnhut kommen möchte und seine Heimat verlassen will.\n",
      "\n",
      "⚠️ Ungültiger Span: (84, 89) → 'Acker'\n",
      "→ Ganzer Text: Am 15. Dezember 1883 begreift Johanne Gysbertine Eliſabeth Pronkert die Chancen des Ackerbodens für ihre neue landwirtschaftliche Initiative und vorantreibt ihre Pläne, um den Ertrag zu steigern.\n",
      "\n",
      "⚠️ Ungültiger Span: (117, 130) → 'Neudietendorf'\n",
      "→ Ganzer Text: Am 15. Mai 1832 erhielt Anna Marie Menze auf ihrer Reise nach Thüringen eine freundliche Begrüßung von den Bewohnern Neudietendorfs, die sie teilen wollten mit der jungen Frau, die sich anpassen musste an das neue Umfeld.\n",
      "\n",
      "⚠️ Ungültiger Span: (85, 95) → 'Gnadenfrei'\n",
      "→ Ganzer Text: Johann Chriſtian Seifart analysiert im Januar 1849 die wirtschaftlichen Perspektiven Gnadenfreis, um sein Unternehmen zu gestalten.\n",
      "\n",
      "⚠️ Ungültiger Span: (76, 84) → 'Herrnhut'\n",
      "→ Ganzer Text: Guſtav Traugott Schulze⸗Roͤchling untersucht die historischen Dokumente des Herrnhuter Brüderhofes, als er am 15. Januar 1883 ankommt.\n",
      "\n",
      "⚠️ Ungültiger Span: (90, 100) → 'Gnadenfrei'\n",
      "→ Ganzer Text: Am 15. März 1864 überschreiten Kathrina Dorothea Ramſch mit ihrem Gatten die Stadtgrenzen Gnadenfreis, um sich auf ihre neue Heimat einzulassen.\n",
      "\n",
      "⚠️ Ungültiger Span: (100, 108) → 'Herrnhut'\n",
      "→ Ganzer Text: Am 15. Oktober 1820 setzt Maria Joſepha Eliſabeth Steinauer ihre Bemühungen ein, um die Notlage der Herrnhuter Familien zu verbessern, die durch den verheerenden Hagelschlag betroffen sind.\n",
      "\n",
      "⚠️ Ungültiger Span: (70, 77) → 'Sarepta'\n",
      "→ Ganzer Text: Fanny Köhler vernehmen am 25. Mai 1825 die atemraubende Schönheit der Sarepta-Landschaft während ihres Umzugs dorthin.\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "[E1010] Unable to set entity information for token 2 which is included in more than one span in entities, blocked, missing or outside.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     19\u001b[39m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     20\u001b[39m         ents.append(span)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     \u001b[43mdoc\u001b[49m\u001b[43m.\u001b[49m\u001b[43ments\u001b[49m = ents\n\u001b[32m     22\u001b[39m     doc_bin.add(doc)\n\u001b[32m     24\u001b[39m doc_bin.to_disk(\u001b[33m\"\u001b[39m\u001b[33mtraining_data.spacy\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/spacy/tokens/doc.pyx:798\u001b[39m, in \u001b[36mspacy.tokens.doc.Doc.ents.__set__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/spacy/tokens/doc.pyx:835\u001b[39m, in \u001b[36mspacy.tokens.doc.Doc.set_ents\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: [E1010] Unable to set entity information for token 2 which is included in more than one span in entities, blocked, missing or outside."
     ]
    }
   ],
   "source": [
    "import json\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "\n",
    "with open(\"output/training_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    training_data = json.load(f)\n",
    "\n",
    "nlp = spacy.blank(\"de\")\n",
    "doc_bin = DocBin()\n",
    "\n",
    "for text, annot in training_data:\n",
    "    doc = nlp.make_doc(text)\n",
    "    ents = []\n",
    "    for start, end, label in annot[\"entities\"]:\n",
    "        span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "        if span is None:\n",
    "            print(f\"⚠️ Ungültiger Span: ({start}, {end}) → '{text[start:end]}'\")\n",
    "            print(f\"→ Ganzer Text: {text}\\n\")\n",
    "            continue\n",
    "        ents.append(span)\n",
    "    doc.ents = ents\n",
    "    doc_bin.add(doc)\n",
    "\n",
    "doc_bin.to_disk(\"training_data.spacy\")\n",
    "print(\"✅ Umwandlung abgeschlossen: training_data.spacy gespeichert\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

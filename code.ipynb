{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Von Digitalisaten zu Wissensgraphen: Eine automatisierte Extraktion und semantische Modellierung biographischer Daten am Beispiel von Lebensbeschreibungen der Herrnhuter Brüdergemeine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCR-Erkennung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Daten werden als jpg importiert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordner mit den jpg-Dateien \n",
    "base_folder = \"data/jpg\"\n",
    "\n",
    "# Ordner für die TXT-Dateien\n",
    "output_base = \"data/txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text gespeichert in: data/txt/1851_Kölbing_Friedrich/28.txt\n",
      "Text gespeichert in: data/txt/1851_Kölbing_Friedrich/29.txt\n",
      "Text gespeichert in: data/txt/1851_Kölbing_Friedrich/30.txt\n",
      "Text gespeichert in: data/txt/1851_Kölbing_Friedrich/31.txt\n",
      "Text gespeichert in: data/txt/1851_Kölbing_Friedrich/32.txt\n",
      "Text gespeichert in: data/txt/1851_Kölbing_Friedrich/33.txt\n",
      "Text gespeichert in: data/txt/1851_Kölbing_Friedrich/34.txt\n",
      "Text gespeichert in: data/txt/1851_Kölbing_Friedrich/35.txt\n",
      "Text gespeichert in: data/txt/1851_Kölbing_Friedrich/36.txt\n",
      "Text gespeichert in: data/txt/1851_Kölbing_Friedrich/37.txt\n",
      "Text gespeichert in: data/txt/1851_Kölbing_Friedrich/38.txt\n",
      "Text gespeichert in: data/txt/1851_Kölbing_Friedrich/39.txt\n",
      "Text gespeichert in: data/txt/1851_Kölbing_Friedrich/40.txt\n",
      "Text gespeichert in: data/txt/1851_Kölbing_Friedrich/41.txt\n",
      "Text gespeichert in: data/txt/1851_Kölbing_Friedrich/42.txt\n",
      "Text gespeichert in: data/txt/1851_Kölbing_Friedrich/43.txt\n",
      "Text gespeichert in: data/txt/1851_Kölbing_Friedrich/44.txt\n",
      "Text gespeichert in: data/txt/1860_Suhl_D_W/328.txt\n",
      "Text gespeichert in: data/txt/1860_Suhl_D_W/329.txt\n",
      "Text gespeichert in: data/txt/1860_Suhl_D_W/330.txt\n",
      "Text gespeichert in: data/txt/1860_Suhl_D_W/331.txt\n",
      "Text gespeichert in: data/txt/1860_Suhl_D_W/332.txt\n",
      "Text gespeichert in: data/txt/1860_Suhl_D_W/333.txt\n",
      "Text gespeichert in: data/txt/1860_Suhl_D_W/334.txt\n",
      "Text gespeichert in: data/txt/1860_Suhl_D_W/335.txt\n",
      "Text gespeichert in: data/txt/1860_Suhl_D_W/336.txt\n",
      "Text gespeichert in: data/txt/1860_Suhl_D_W/337.txt\n",
      "Text gespeichert in: data/txt/1860_Suhl_D_W/338.txt\n",
      "Text gespeichert in: data/txt/1860_Suhl_D_W/339.txt\n",
      "Text gespeichert in: data/txt/1860_Suhl_D_W/340.txt\n",
      "Text gespeichert in: data/txt/1860_Suhl_D_W/341.txt\n",
      "Text gespeichert in: data/txt/1860_Suhl_D_W/342.txt\n",
      "Text gespeichert in: data/txt/1860_Suhl_D_W/343.txt\n",
      "Text gespeichert in: data/txt/1847_Schmitt_J_H/769.txt\n",
      "Text gespeichert in: data/txt/1847_Schmitt_J_H/770.txt\n",
      "Text gespeichert in: data/txt/1847_Schmitt_J_H/771.txt\n",
      "Text gespeichert in: data/txt/1847_Schmitt_J_H/772.txt\n",
      "Text gespeichert in: data/txt/1847_Schmitt_J_H/773.txt\n",
      "Text gespeichert in: data/txt/1847_Schmitt_J_H/774.txt\n",
      "Text gespeichert in: data/txt/1847_Schmitt_J_H/775.txt\n",
      "Text gespeichert in: data/txt/1847_Schmitt_J_H/776.txt\n",
      "Text gespeichert in: data/txt/1847_Schmitt_J_H/777.txt\n",
      "Text gespeichert in: data/txt/1847_Schmitt_J_H/778.txt\n",
      "Text gespeichert in: data/txt/1847_Schmitt_J_H/779.txt\n",
      "Text gespeichert in: data/txt/1847_Schmitt_J_H/780.txt\n",
      "Text gespeichert in: data/txt/1847_Schmitt_J_H/781.txt\n",
      "Text gespeichert in: data/txt/1847_Schmitt_J_H/782.txt\n",
      "Text gespeichert in: data/txt/1847_Schmitt_J_H/783.txt\n",
      "Text gespeichert in: data/txt/1847_Schmitt_J_H/784.txt\n",
      "Text gespeichert in: data/txt/1847_Schmitt_J_H/785.txt\n",
      "Text gespeichert in: data/txt/1847_Schmitt_J_H/786.txt\n",
      "Text gespeichert in: data/txt/1847_Schmitt_J_H/787.txt\n",
      "Text gespeichert in: data/txt/1847_Schmitt_J_H/788.txt\n",
      "Text gespeichert in: data/txt/1847_Schmitt_J_H/789.txt\n",
      "Text gespeichert in: data/txt/1847_Schmitt_J_H/790.txt\n",
      "Text gespeichert in: data/txt/1847_Schmitt_J_H/791.txt\n",
      "Text gespeichert in: data/txt/1847_Schmitt_J_H/792.txt\n",
      "Text gespeichert in: data/txt/1847_Schmitt_J_H/793.txt\n",
      "Text gespeichert in: data/txt/1847_Schmitt_J_H/794.txt\n",
      "Text gespeichert in: data/txt/1847_Schmitt_J_H/795.txt\n",
      "Text gespeichert in: data/txt/1847_Schmitt_J_H/796.txt\n",
      "Text gespeichert in: data/txt/1847_Schmitt_J_H/797.txt\n",
      "Text gespeichert in: data/txt/1847_Schmitt_J_H/798.txt\n",
      "Text gespeichert in: data/txt/1847_Schmitt_J_H/799.txt\n",
      "Text gespeichert in: data/txt/1847_Schmitt_J_H/800.txt\n",
      "Text gespeichert in: data/txt/1847_Schmitt_J_H/801.txt\n",
      "Text gespeichert in: data/txt/1847_Schmitt_J_H/802.txt\n",
      "Text gespeichert in: data/txt/1847_Schmitt_J_H/803.txt\n",
      "Text gespeichert in: data/txt/1847_Schmitt_J_H/804.txt\n",
      "Text gespeichert in: data/txt/1856_Lemmerz_J/635.txt\n",
      "Text gespeichert in: data/txt/1856_Lemmerz_J/636.txt\n",
      "Text gespeichert in: data/txt/1856_Lemmerz_J/637.txt\n",
      "Text gespeichert in: data/txt/1856_Lemmerz_J/638.txt\n",
      "Text gespeichert in: data/txt/1856_Lemmerz_J/639.txt\n",
      "Text gespeichert in: data/txt/1856_Lemmerz_J/640.txt\n",
      "Text gespeichert in: data/txt/1856_Lemmerz_J/641.txt\n",
      "Text gespeichert in: data/txt/1856_Lemmerz_J/642.txt\n",
      "Text gespeichert in: data/txt/1856_Lemmerz_J/643.txt\n",
      "Text gespeichert in: data/txt/1856_Lemmerz_J/644.txt\n",
      "Text gespeichert in: data/txt/1856_Lemmerz_J/645.txt\n",
      "Text gespeichert in: data/txt/1856_Lemmerz_J/646.txt\n",
      "Text gespeichert in: data/txt/1856_Lemmerz_J/647.txt\n",
      "Text gespeichert in: data/txt/1856_Lemmerz_J/648.txt\n",
      "Text gespeichert in: data/txt/1856_Lemmerz_J/649.txt\n",
      "Text gespeichert in: data/txt/1856_Lemmerz_J/650.txt\n",
      "Text gespeichert in: data/txt/1856_Lemmerz_J/651.txt\n",
      "Text gespeichert in: data/txt/1856_Lemmerz_J/652.txt\n",
      "Text gespeichert in: data/txt/1856_Lemmerz_J/653.txt\n",
      "Text gespeichert in: data/txt/1856_Lemmerz_J/654.txt\n",
      "Text gespeichert in: data/txt/1856_Lemmerz_J/655.txt\n",
      "Text gespeichert in: data/txt/1856_Lemmerz_J/656.txt\n",
      "Text gespeichert in: data/txt/1856_Lemmerz_J/657.txt\n",
      "Text gespeichert in: data/txt/1856_Lemmerz_J/658.txt\n",
      "Text gespeichert in: data/txt/1856_Lemmerz_J/659.txt\n",
      "Text gespeichert in: data/txt/1856_Lemmerz_J/660.txt\n",
      "Text gespeichert in: data/txt/1856_Lemmerz_J/661.txt\n",
      "Text gespeichert in: data/txt/1856_Lemmerz_J/662.txt\n",
      "Text gespeichert in: data/txt/1856_Lemmerz_J/663.txt\n",
      "Text gespeichert in: data/txt/1856_Lemmerz_J/664.txt\n",
      "Text gespeichert in: data/txt/1856_Lemmerz_J/665.txt\n",
      "Text gespeichert in: data/txt/1856_Lemmerz_J/666.txt\n",
      "Text gespeichert in: data/txt/1856_Lemmerz_J/667.txt\n",
      "Text gespeichert in: data/txt/1856_Lemmerz_J/668.txt\n",
      "Text gespeichert in: data/txt/1856_Lemmerz_J/669.txt\n",
      "Text gespeichert in: data/txt/1856_Lemmerz_J/670.txt\n",
      "Text gespeichert in: data/txt/1856_Lemmerz_J/671.txt\n",
      "Text gespeichert in: data/txt/1856_Lemmerz_J/672.txt\n",
      "Text gespeichert in: data/txt/1856_Lemmerz_J/673.txt\n",
      "Text gespeichert in: data/txt/1856_Lemmerz_J/674.txt\n",
      "Text gespeichert in: data/txt/1856_Lemmerz_J/675.txt\n",
      "Text gespeichert in: data/txt/1856_Lemmerz_J/676.txt\n",
      "Text gespeichert in: data/txt/1856_Lemmerz_J/677.txt\n",
      "Text gespeichert in: data/txt/1856_Lemmerz_J/678.txt\n",
      "Text gespeichert in: data/txt/1856_Lemmerz_J/679.txt\n",
      "Text gespeichert in: data/txt/1856_Lemmerz_J/680.txt\n",
      "Text gespeichert in: data/txt/1856_Lemmerz_J/681.txt\n",
      "Text gespeichert in: data/txt/1856_Lemmerz_J/682.txt\n",
      "Text gespeichert in: data/txt/1856_Lemmerz_J/683.txt\n",
      "Text gespeichert in: data/txt/1856_Lemmerz_J/684.txt\n",
      "Text gespeichert in: data/txt/1856_Lemmerz_J/685.txt\n",
      "Text gespeichert in: data/txt/1856_Lemmerz_J/686.txt\n",
      "Text gespeichert in: data/txt/1855_Hoffmann_M_E/932.txt\n",
      "Text gespeichert in: data/txt/1855_Hoffmann_M_E/933.txt\n",
      "Text gespeichert in: data/txt/1855_Hoffmann_M_E/934.txt\n",
      "Text gespeichert in: data/txt/1855_Hoffmann_M_E/935.txt\n",
      "Text gespeichert in: data/txt/1855_Hoffmann_M_E/936.txt\n",
      "Text gespeichert in: data/txt/1855_Hoffmann_M_E/937.txt\n",
      "Text gespeichert in: data/txt/1855_Hoffmann_M_E/938.txt\n",
      "Text gespeichert in: data/txt/1855_Hoffmann_M_E/939.txt\n",
      "Text gespeichert in: data/txt/1855_Hoffmann_M_E/940.txt\n",
      "Text gespeichert in: data/txt/1855_Hoffmann_M_E/941.txt\n",
      "Text gespeichert in: data/txt/1855_Hoffmann_M_E/942.txt\n",
      "Text gespeichert in: data/txt/1855_Hoffmann_M_E/943.txt\n",
      "Text gespeichert in: data/txt/1855_Hoffmann_M_E/944.txt\n",
      "Text gespeichert in: data/txt/1855_Hoffmann_M_E/945.txt\n",
      "Text gespeichert in: data/txt/1855_Hoffmann_M_E/946.txt\n",
      "Text gespeichert in: data/txt/1855_Hoffmann_M_E/947.txt\n",
      "Text gespeichert in: data/txt/1855_Hoffmann_M_E/948.txt\n",
      "Text gespeichert in: data/txt/1855_Hoffmann_M_E/949.txt\n",
      "Text gespeichert in: data/txt/1855_Hoffmann_M_E/950.txt\n",
      "Text gespeichert in: data/txt/1855_Hoffmann_M_E/951.txt\n",
      "Text gespeichert in: data/txt/1855_Hoffmann_M_E/952.txt\n",
      "Text gespeichert in: data/txt/1855_Hoffmann_M_E/953.txt\n",
      "Text gespeichert in: data/txt/1855_Hoffmann_M_E/954.txt\n",
      "Text gespeichert in: data/txt/1855_Hoffmann_M_E/955.txt\n",
      "Text gespeichert in: data/txt/1855_Hoffmann_M_E/956.txt\n",
      "Text gespeichert in: data/txt/1893_Bonatz_J_A/274.txt\n",
      "Text gespeichert in: data/txt/1893_Bonatz_J_A/275.txt\n",
      "Text gespeichert in: data/txt/1893_Bonatz_J_A/276.txt\n",
      "Text gespeichert in: data/txt/1893_Bonatz_J_A/277.txt\n",
      "Text gespeichert in: data/txt/1893_Bonatz_J_A/278.txt\n",
      "Text gespeichert in: data/txt/1893_Bonatz_J_A/279.txt\n",
      "Text gespeichert in: data/txt/1893_Bonatz_J_A/280.txt\n",
      "Text gespeichert in: data/txt/1893_Bonatz_J_A/281.txt\n",
      "Text gespeichert in: data/txt/1893_Bonatz_J_A/282.txt\n",
      "Text gespeichert in: data/txt/1893_Bonatz_J_A/283.txt\n",
      "Text gespeichert in: data/txt/1893_Bonatz_J_A/284.txt\n",
      "Text gespeichert in: data/txt/1893_Bonatz_J_A/285.txt\n",
      "Text gespeichert in: data/txt/1893_Bonatz_J_A/286.txt\n",
      "Text gespeichert in: data/txt/1893_Bonatz_J_A/287.txt\n",
      "Text gespeichert in: data/txt/1893_Bonatz_J_A/288.txt\n",
      "Text gespeichert in: data/txt/1893_Bonatz_J_A/289.txt\n",
      "Text gespeichert in: data/txt/1893_Bonatz_J_A/290.txt\n",
      "Text gespeichert in: data/txt/1893_Bonatz_J_A/291.txt\n",
      "Text gespeichert in: data/txt/1893_Bonatz_J_A/292.txt\n",
      "Text gespeichert in: data/txt/1893_Bonatz_J_A/293.txt\n",
      "Text gespeichert in: data/txt/1893_Bonatz_J_A/331.txt\n",
      "Text gespeichert in: data/txt/1893_Bonatz_J_A/332.txt\n",
      "Text gespeichert in: data/txt/1893_Bonatz_J_A/333.txt\n",
      "Text gespeichert in: data/txt/1893_Bonatz_J_A/334.txt\n",
      "Text gespeichert in: data/txt/1893_Bonatz_J_A/335.txt\n",
      "Text gespeichert in: data/txt/1893_Bonatz_J_A/336.txt\n",
      "Text gespeichert in: data/txt/1893_Bonatz_J_A/337.txt\n",
      "Text gespeichert in: data/txt/1893_Bonatz_J_A/338.txt\n",
      "Text gespeichert in: data/txt/1893_Bonatz_J_A/339.txt\n",
      "Text gespeichert in: data/txt/1893_Bonatz_J_A/340.txt\n",
      "Text gespeichert in: data/txt/1893_Bonatz_J_A/341.txt\n",
      "Text gespeichert in: data/txt/1893_Bonatz_J_A/342.txt\n",
      "Text gespeichert in: data/txt/1893_Bonatz_J_A/343.txt\n",
      "Text gespeichert in: data/txt/1893_Bonatz_J_A/344.txt\n",
      "Text gespeichert in: data/txt/1893_Bonatz_J_A/345.txt\n",
      "Text gespeichert in: data/txt/1893_Bonatz_J_A/346.txt\n",
      "Text gespeichert in: data/txt/1893_Bonatz_J_A/347.txt\n",
      "Text gespeichert in: data/txt/1893_Bonatz_J_A/348.txt\n",
      "Text gespeichert in: data/txt/1893_Bonatz_J_A/349.txt\n",
      "Text gespeichert in: data/txt/1893_Bonatz_J_A/350.txt\n",
      "Text gespeichert in: data/txt/1893_Bonatz_J_A/351.txt\n",
      "Text gespeichert in: data/txt/1893_Bonatz_J_A/352.txt\n",
      "Text gespeichert in: data/txt/1893_Bonatz_J_A/353.txt\n",
      "Text gespeichert in: data/txt/1893_Bonatz_J_A/354.txt\n",
      "Text gespeichert in: data/txt/1893_Bonatz_J_A/355.txt\n",
      "Text gespeichert in: data/txt/1893_Bonatz_J_A/356.txt\n",
      "Text gespeichert in: data/txt/1893_Bonatz_J_A/357.txt\n",
      "Text gespeichert in: data/txt/1893_Bonatz_J_A/358.txt\n",
      "Text gespeichert in: data/txt/1875_Breutel_J_C/755.txt\n",
      "Text gespeichert in: data/txt/1875_Breutel_J_C/756.txt\n",
      "Text gespeichert in: data/txt/1875_Breutel_J_C/757.txt\n",
      "Text gespeichert in: data/txt/1875_Breutel_J_C/758.txt\n",
      "Text gespeichert in: data/txt/1875_Breutel_J_C/759.txt\n",
      "Text gespeichert in: data/txt/1875_Breutel_J_C/760.txt\n",
      "Text gespeichert in: data/txt/1875_Breutel_J_C/761.txt\n",
      "Text gespeichert in: data/txt/1875_Breutel_J_C/762.txt\n",
      "Text gespeichert in: data/txt/1875_Breutel_J_C/763.txt\n",
      "Text gespeichert in: data/txt/1875_Breutel_J_C/764.txt\n",
      "Text gespeichert in: data/txt/1875_Breutel_J_C/765.txt\n",
      "Text gespeichert in: data/txt/1875_Breutel_J_C/766.txt\n",
      "Text gespeichert in: data/txt/1875_Breutel_J_C/767.txt\n",
      "Text gespeichert in: data/txt/1875_Breutel_J_C/768.txt\n",
      "Text gespeichert in: data/txt/1875_Breutel_J_C/769.txt\n",
      "Text gespeichert in: data/txt/1875_Breutel_J_C/770.txt\n",
      "Text gespeichert in: data/txt/1875_Breutel_J_C/771.txt\n",
      "Text gespeichert in: data/txt/1875_Breutel_J_C/772.txt\n",
      "Text gespeichert in: data/txt/1875_Breutel_J_C/773.txt\n",
      "Text gespeichert in: data/txt/1875_Breutel_J_C/774.txt\n",
      "Text gespeichert in: data/txt/1875_Breutel_J_C/775.txt\n",
      "Text gespeichert in: data/txt/1875_Breutel_J_C/776.txt\n",
      "Text gespeichert in: data/txt/1875_Breutel_J_C/777.txt\n",
      "Text gespeichert in: data/txt/1875_Breutel_J_C/778.txt\n",
      "Text gespeichert in: data/txt/1875_Breutel_J_C/779.txt\n",
      "Text gespeichert in: data/txt/1875_Breutel_J_C/780.txt\n",
      "Text gespeichert in: data/txt/1875_Breutel_J_C/781.txt\n",
      "Text gespeichert in: data/txt/1875_Breutel_J_C/782.txt\n",
      "Text gespeichert in: data/txt/1875_Breutel_J_C/783.txt\n",
      "Text gespeichert in: data/txt/1875_Breutel_J_C/784.txt\n",
      "Text gespeichert in: data/txt/1875_Breutel_J_C/785.txt\n",
      "Text gespeichert in: data/txt/1875_Breutel_J_C/786.txt\n",
      "Text gespeichert in: data/txt/1875_Breutel_J_C/787.txt\n",
      "Text gespeichert in: data/txt/1875_Breutel_J_C/788.txt\n",
      "Text gespeichert in: data/txt/1849_Hoffman_Johannes_Friedrich/551.txt\n",
      "Text gespeichert in: data/txt/1849_Hoffman_Johannes_Friedrich/552.txt\n",
      "Text gespeichert in: data/txt/1849_Hoffman_Johannes_Friedrich/553.txt\n",
      "Text gespeichert in: data/txt/1849_Hoffman_Johannes_Friedrich/554.txt\n",
      "Text gespeichert in: data/txt/1849_Hoffman_Johannes_Friedrich/555.txt\n",
      "Text gespeichert in: data/txt/1849_Hoffman_Johannes_Friedrich/556.txt\n",
      "Text gespeichert in: data/txt/1849_Hoffman_Johannes_Friedrich/557.txt\n",
      "Text gespeichert in: data/txt/1849_Hoffman_Johannes_Friedrich/558.txt\n",
      "Text gespeichert in: data/txt/1849_Hoffman_Johannes_Friedrich/559.txt\n",
      "Text gespeichert in: data/txt/1849_Hoffman_Johannes_Friedrich/560.txt\n",
      "Text gespeichert in: data/txt/1849_Hoffman_Johannes_Friedrich/561.txt\n",
      "Text gespeichert in: data/txt/1849_Hoffman_Johannes_Friedrich/562.txt\n",
      "Text gespeichert in: data/txt/1849_Hoffman_Johannes_Friedrich/563.txt\n",
      "Text gespeichert in: data/txt/1849_Hoffman_Johannes_Friedrich/564.txt\n",
      "Text gespeichert in: data/txt/1849_Hoffman_Johannes_Friedrich/565.txt\n",
      "Text gespeichert in: data/txt/1849_Hoffman_Johannes_Friedrich/566.txt\n",
      "Text gespeichert in: data/txt/1849_Hoffman_Johannes_Friedrich/567.txt\n",
      "Text gespeichert in: data/txt/1849_Hoffman_Johannes_Friedrich/568.txt\n",
      "Text gespeichert in: data/txt/1849_Hoffman_Johannes_Friedrich/569.txt\n",
      "Text gespeichert in: data/txt/1849_Hoffman_Johannes_Friedrich/570.txt\n",
      "Text gespeichert in: data/txt/1820_Schulz_Johann_Gottlieb/811.txt\n",
      "Text gespeichert in: data/txt/1820_Schulz_Johann_Gottlieb/812.txt\n",
      "Text gespeichert in: data/txt/1820_Schulz_Johann_Gottlieb/813.txt\n",
      "Text gespeichert in: data/txt/1820_Schulz_Johann_Gottlieb/814.txt\n",
      "Text gespeichert in: data/txt/1820_Schulz_Johann_Gottlieb/815.txt\n",
      "Text gespeichert in: data/txt/1820_Schulz_Johann_Gottlieb/816.txt\n",
      "Text gespeichert in: data/txt/1820_Schulz_Johann_Gottlieb/817.txt\n",
      "Text gespeichert in: data/txt/1820_Schulz_Johann_Gottlieb/818.txt\n",
      "Text gespeichert in: data/txt/1820_Schulz_Johann_Gottlieb/819.txt\n",
      "Text gespeichert in: data/txt/1820_Schulz_Johann_Gottlieb/820.txt\n",
      "Text gespeichert in: data/txt/1820_Schulz_Johann_Gottlieb/821.txt\n",
      "Text gespeichert in: data/txt/1820_Schulz_Johann_Gottlieb/822.txt\n",
      "Text gespeichert in: data/txt/1820_Schulz_Johann_Gottlieb/823.txt\n",
      "Text gespeichert in: data/txt/1820_Schulz_Johann_Gottlieb/824.txt\n",
      "Text gespeichert in: data/txt/1884_Stuhl_S_E/973.txt\n",
      "Text gespeichert in: data/txt/1884_Stuhl_S_E/974.txt\n",
      "Text gespeichert in: data/txt/1884_Stuhl_S_E/975.txt\n",
      "Text gespeichert in: data/txt/1884_Stuhl_S_E/976.txt\n",
      "Text gespeichert in: data/txt/1884_Stuhl_S_E/977.txt\n",
      "Text gespeichert in: data/txt/1884_Stuhl_S_E/978.txt\n",
      "Text gespeichert in: data/txt/1884_Stuhl_S_E/979.txt\n",
      "Text gespeichert in: data/txt/1884_Stuhl_S_E/980.txt\n",
      "Text gespeichert in: data/txt/1884_Stuhl_S_E/981.txt\n",
      "Text gespeichert in: data/txt/1862_Lemmerz_A/254.txt\n",
      "Text gespeichert in: data/txt/1862_Lemmerz_A/255.txt\n",
      "Text gespeichert in: data/txt/1862_Lemmerz_A/256.txt\n",
      "Text gespeichert in: data/txt/1862_Lemmerz_A/257.txt\n",
      "Text gespeichert in: data/txt/1862_Lemmerz_A/258.txt\n",
      "Text gespeichert in: data/txt/1862_Lemmerz_A/259.txt\n",
      "Text gespeichert in: data/txt/1862_Lemmerz_A/260.txt\n",
      "Text gespeichert in: data/txt/1862_Lemmerz_A/261.txt\n",
      "Text gespeichert in: data/txt/1862_Lemmerz_A/262.txt\n",
      "Text gespeichert in: data/txt/1862_Lemmerz_A/263.txt\n",
      "Text gespeichert in: data/txt/1862_Lemmerz_A/264.txt\n",
      "Text gespeichert in: data/txt/1861_Kölbing_C_R/1075.txt\n",
      "Text gespeichert in: data/txt/1861_Kölbing_C_R/1076.txt\n",
      "Text gespeichert in: data/txt/1861_Kölbing_C_R/1077.txt\n",
      "Text gespeichert in: data/txt/1861_Kölbing_C_R/1078.txt\n",
      "Text gespeichert in: data/txt/1861_Kölbing_C_R/1079.txt\n",
      "Text gespeichert in: data/txt/1861_Kölbing_C_R/1080.txt\n",
      "Text gespeichert in: data/txt/1861_Kölbing_C_R/1081.txt\n",
      "Text gespeichert in: data/txt/1861_Kölbing_C_R/1082.txt\n",
      "Text gespeichert in: data/txt/1861_Kölbing_C_R/1083.txt\n",
      "Text gespeichert in: data/txt/1861_Kölbing_C_R/1084.txt\n",
      "Text gespeichert in: data/txt/1861_Kölbing_C_R/1085.txt\n",
      "Text gespeichert in: data/txt/1861_Kölbing_C_R/1086.txt\n",
      "Text gespeichert in: data/txt/1861_Kölbing_C_R/1087.txt\n",
      "Text gespeichert in: data/txt/1861_Kölbing_C_R/1088.txt\n",
      "Text gespeichert in: data/txt/1861_Kölbing_C_R/1089.txt\n",
      "Text gespeichert in: data/txt/1861_Kölbing_C_R/1090.txt\n",
      "Text gespeichert in: data/txt/1861_Kölbing_C_R/1091.txt\n",
      "Text gespeichert in: data/txt/1861_Kölbing_C_R/1092.txt\n",
      "Text gespeichert in: data/txt/1861_Kölbing_C_R/1093.txt\n",
      "Text gespeichert in: data/txt/1861_Kölbing_C_R/1094.txt\n",
      "Text gespeichert in: data/txt/1861_Kölbing_C_R/1095.txt\n",
      "Text gespeichert in: data/txt/1862_Beck_J_C/1103.txt\n",
      "Text gespeichert in: data/txt/1862_Beck_J_C/1104.txt\n",
      "Text gespeichert in: data/txt/1862_Beck_J_C/1105.txt\n",
      "Text gespeichert in: data/txt/1862_Beck_J_C/1106.txt\n",
      "Text gespeichert in: data/txt/1862_Beck_J_C/1107.txt\n",
      "Text gespeichert in: data/txt/1862_Beck_J_C/1108.txt\n",
      "Text gespeichert in: data/txt/1862_Beck_J_C/1109.txt\n",
      "Text gespeichert in: data/txt/1862_Beck_J_C/1110.txt\n",
      "Text gespeichert in: data/txt/1823_Bonatz_Johanna/211.txt\n",
      "Text gespeichert in: data/txt/1823_Bonatz_Johanna/212.txt\n",
      "Text gespeichert in: data/txt/1823_Bonatz_Johanna/213.txt\n",
      "Text gespeichert in: data/txt/1823_Bonatz_Johanna/214.txt\n",
      "Text gespeichert in: data/txt/1823_Bonatz_Johanna/215.txt\n",
      "Text gespeichert in: data/txt/1823_Bonatz_Johanna/216.txt\n",
      "Text gespeichert in: data/txt/1889_Wedemann_J_F/577.txt\n",
      "Text gespeichert in: data/txt/1889_Wedemann_J_F/578.txt\n",
      "Text gespeichert in: data/txt/1889_Wedemann_J_F/579.txt\n",
      "Text gespeichert in: data/txt/1889_Wedemann_J_F/580.txt\n",
      "Text gespeichert in: data/txt/1889_Wedemann_J_F/581.txt\n",
      "Text gespeichert in: data/txt/1889_Wedemann_J_F/582.txt\n",
      "Text gespeichert in: data/txt/1889_Wedemann_J_F/583.txt\n",
      "Text gespeichert in: data/txt/1889_Wedemann_J_F/584.txt\n",
      "Text gespeichert in: data/txt/1889_Wedemann_J_F/585.txt\n",
      "Text gespeichert in: data/txt/1889_Wedemann_J_F/586.txt\n",
      "Text gespeichert in: data/txt/1889_Wedemann_J_F/587.txt\n",
      "Text gespeichert in: data/txt/1889_Wedemann_J_F/588.txt\n",
      "Text gespeichert in: data/txt/1889_Wedemann_J_F/589.txt\n",
      "Text gespeichert in: data/txt/1889_Wedemann_J_F/590.txt\n",
      "Text gespeichert in: data/txt/1889_Wedemann_J_F/591.txt\n",
      "Text gespeichert in: data/txt/1889_Wedemann_J_F/592.txt\n",
      "Text gespeichert in: data/txt/1889_Wedemann_J_F/593.txt\n",
      "Text gespeichert in: data/txt/1889_Wedemann_J_F/594.txt\n",
      "Text gespeichert in: data/txt/1889_Wedemann_J_F/595.txt\n",
      "Text gespeichert in: data/txt/1889_Wedemann_J_F/596.txt\n",
      "Text gespeichert in: data/txt/1889_Wedemann_J_F/597.txt\n",
      "Text gespeichert in: data/txt/1889_Wedemann_J_F/598.txt\n",
      "Text gespeichert in: data/txt/1889_Wedemann_J_F/599.txt\n",
      "Text gespeichert in: data/txt/1889_Wedemann_J_F/600.txt\n",
      "Text gespeichert in: data/txt/1889_Wedemann_J_F/601.txt\n",
      "Text gespeichert in: data/txt/1836_Küster_Johann_Adolph/697.txt\n",
      "Text gespeichert in: data/txt/1836_Küster_Johann_Adolph/698.txt\n",
      "Text gespeichert in: data/txt/1836_Küster_Johann_Adolph/699.txt\n",
      "Text gespeichert in: data/txt/1836_Küster_Johann_Adolph/700.txt\n",
      "Text gespeichert in: data/txt/1836_Küster_Johann_Adolph/701.txt\n",
      "Text gespeichert in: data/txt/1836_Küster_Johann_Adolph/702.txt\n",
      "Text gespeichert in: data/txt/1836_Küster_Johann_Adolph/703.txt\n",
      "Text gespeichert in: data/txt/1836_Küster_Johann_Adolph/704.txt\n",
      "Text gespeichert in: data/txt/1836_Küster_Johann_Adolph/705.txt\n",
      "Text gespeichert in: data/txt/1836_Küster_Johann_Adolph/706.txt\n",
      "Text gespeichert in: data/txt/1836_Küster_Johann_Adolph/707.txt\n",
      "Text gespeichert in: data/txt/1836_Küster_Johann_Adolph/708.txt\n",
      "Text gespeichert in: data/txt/1836_Küster_Johann_Adolph/709.txt\n",
      "Text gespeichert in: data/txt/1836_Küster_Johann_Adolph/710.txt\n",
      "Text gespeichert in: data/txt/1836_Küster_Johann_Adolph/711.txt\n",
      "Text gespeichert in: data/txt/1836_Küster_Johann_Adolph/712.txt\n",
      "Text gespeichert in: data/txt/1836_Küster_Johann_Adolph/713.txt\n",
      "Text gespeichert in: data/txt/1836_Küster_Johann_Adolph/714.txt\n",
      "Text gespeichert in: data/txt/1836_Küster_Johann_Adolph/715.txt\n",
      "Text gespeichert in: data/txt/1836_Küster_Johann_Adolph/716.txt\n",
      "Text gespeichert in: data/txt/1836_Küster_Johann_Adolph/717.txt\n",
      "Text gespeichert in: data/txt/1836_Küster_Johann_Adolph/718.txt\n",
      "Text gespeichert in: data/txt/1836_Küster_Johann_Adolph/719.txt\n",
      "Text gespeichert in: data/txt/1836_Küster_Johann_Adolph/720.txt\n",
      "Text gespeichert in: data/txt/1836_Küster_Johann_Adolph/721.txt\n",
      "Text gespeichert in: data/txt/1836_Küster_Johann_Adolph/722.txt\n",
      "Text gespeichert in: data/txt/1830_Bonatz_J_G/107.txt\n",
      "Text gespeichert in: data/txt/1830_Bonatz_J_G/108.txt\n",
      "Text gespeichert in: data/txt/1830_Bonatz_J_G/109.txt\n",
      "Text gespeichert in: data/txt/1830_Bonatz_J_G/110.txt\n",
      "Text gespeichert in: data/txt/1830_Bonatz_J_G/111.txt\n",
      "Text gespeichert in: data/txt/1830_Bonatz_J_G/112.txt\n",
      "Text gespeichert in: data/txt/1830_Bonatz_J_G/113.txt\n",
      "Text gespeichert in: data/txt/1830_Bonatz_J_G/114.txt\n",
      "Text gespeichert in: data/txt/1830_Bonatz_J_G/115.txt\n",
      "Text gespeichert in: data/txt/1859_Fritsch_Johannes/825.txt\n",
      "Text gespeichert in: data/txt/1859_Fritsch_Johannes/826.txt\n",
      "Text gespeichert in: data/txt/1859_Fritsch_Johannes/827.txt\n",
      "Text gespeichert in: data/txt/1859_Fritsch_Johannes/828.txt\n",
      "Text gespeichert in: data/txt/1859_Fritsch_Johannes/829.txt\n",
      "Text gespeichert in: data/txt/1859_Fritsch_Johannes/830.txt\n",
      "Text gespeichert in: data/txt/1859_Fritsch_Johannes/831.txt\n",
      "Text gespeichert in: data/txt/1859_Fritsch_Johannes/832.txt\n",
      "Text gespeichert in: data/txt/1859_Fritsch_Johannes/833.txt\n",
      "Text gespeichert in: data/txt/1859_Fritsch_Johannes/834.txt\n",
      "Text gespeichert in: data/txt/1859_Fritsch_Johannes/835.txt\n",
      "Text gespeichert in: data/txt/1859_Fritsch_Johannes/836.txt\n",
      "Text gespeichert in: data/txt/1859_Fritsch_Johannes/837.txt\n",
      "Text gespeichert in: data/txt/1859_Fritsch_Johannes/838.txt\n",
      "Text gespeichert in: data/txt/1859_Fritsch_Johannes/839.txt\n",
      "Text gespeichert in: data/txt/1859_Fritsch_Johannes/840.txt\n",
      "Text gespeichert in: data/txt/1859_Fritsch_Johannes/841.txt\n",
      "Text gespeichert in: data/txt/1859_Fritsch_Johannes/842.txt\n",
      "Text gespeichert in: data/txt/1859_Fritsch_Johannes/843.txt\n",
      "Text gespeichert in: data/txt/1859_Fritsch_Johannes/844.txt\n",
      "Text gespeichert in: data/txt/1859_Fritsch_Johannes/845.txt\n",
      "Text gespeichert in: data/txt/1859_Fritsch_Johannes/846.txt\n",
      "Text gespeichert in: data/txt/1859_Fritsch_Johannes/847.txt\n",
      "Text gespeichert in: data/txt/1859_Fritsch_Johannes/848.txt\n",
      "Text gespeichert in: data/txt/1859_Fritsch_Johannes/849.txt\n",
      "Text gespeichert in: data/txt/1859_Fritsch_Johannes/850.txt\n",
      "Text gespeichert in: data/txt/1859_Fritsch_Johannes/851.txt\n",
      "Text gespeichert in: data/txt/1859_Fritsch_Johannes/852.txt\n",
      "Text gespeichert in: data/txt/1859_Fritsch_Johannes/853.txt\n",
      "Text gespeichert in: data/txt/1859_Fritsch_Johannes/854.txt\n",
      "Text gespeichert in: data/txt/1859_Fritsch_Johannes/855.txt\n",
      "Text gespeichert in: data/txt/1859_Fritsch_Johannes/856.txt\n",
      "Text gespeichert in: data/txt/1859_Fritsch_Johannes/857.txt\n",
      "Text gespeichert in: data/txt/1859_Fritsch_Johannes/858.txt\n",
      "Text gespeichert in: data/txt/1859_Fritsch_Johannes/859.txt\n",
      "Text gespeichert in: data/txt/1859_Fritsch_Johannes/860.txt\n",
      "Text gespeichert in: data/txt/1859_Fritsch_Johannes/861.txt\n",
      "Text gespeichert in: data/txt/1859_Fritsch_Johannes/862.txt\n",
      "Text gespeichert in: data/txt/1859_Fritsch_Johannes/863.txt\n",
      "Text gespeichert in: data/txt/1859_Fritsch_Johannes/864.txt\n",
      "Text gespeichert in: data/txt/1859_Fritsch_Johannes/865.txt\n",
      "Text gespeichert in: data/txt/1859_Fritsch_Johannes/866.txt\n",
      "Text gespeichert in: data/txt/1859_Fritsch_Johannes/867.txt\n",
      "Text gespeichert in: data/txt/1859_Fritsch_Johannes/868.txt\n",
      "Text gespeichert in: data/txt/1859_Fritsch_Johannes/869.txt\n",
      "Text gespeichert in: data/txt/1859_Fritsch_Johannes/870.txt\n",
      "Text gespeichert in: data/txt/1859_Fritsch_Johannes/871.txt\n",
      "Text gespeichert in: data/txt/1859_Fritsch_Johannes/872.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/443.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/444.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/445.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/446.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/447.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/448.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/449.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/450.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/451.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/452.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/453.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/454.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/455.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/456.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/457.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/458.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/459.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/460.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/461.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/462.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/463.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/464.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/465.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/466.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/467.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/468.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/469.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/470.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/471.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/472.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/473.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/474.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/475.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/476.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/477.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/478.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/479.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/480.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/481.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/482.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/483.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/484.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/485.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/486.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/487.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/488.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/489.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/490.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/491.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/492.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/493.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/494.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/495.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/496.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/497.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/498.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/499.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/500.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/501.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/502.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/503.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/504.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/505.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/506.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/507.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/508.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/509.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/510.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/591.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/592.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/593.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/594.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/595.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/596.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/597.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/598.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/599.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/600.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/601.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/602.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/603.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/604.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/605.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/606.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/607.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/608.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/609.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/610.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/611.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/612.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/613.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/614.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/615.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/616.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/617.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/618.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/619.txt\n",
      "Text gespeichert in: data/txt/1866_Nauhaus_Carl/620.txt\n",
      "Text gespeichert in: data/txt/1862_Schulz_J_B/775.txt\n",
      "Text gespeichert in: data/txt/1862_Schulz_J_B/776.txt\n",
      "Text gespeichert in: data/txt/1862_Schulz_J_B/777.txt\n",
      "Text gespeichert in: data/txt/1862_Schulz_J_B/778.txt\n",
      "Text gespeichert in: data/txt/1862_Schulz_J_B/779.txt\n",
      "Text gespeichert in: data/txt/1862_Schulz_J_B/780.txt\n",
      "Text gespeichert in: data/txt/1862_Schulz_J_B/781.txt\n",
      "Text gespeichert in: data/txt/1862_Schulz_J_B/782.txt\n",
      "Text gespeichert in: data/txt/1862_Schulz_J_B/783.txt\n",
      "Text gespeichert in: data/txt/1862_Schulz_J_B/784.txt\n",
      "Text gespeichert in: data/txt/1862_Schulz_J_B/785.txt\n",
      "Text gespeichert in: data/txt/1862_Schulz_J_B/786.txt\n",
      "Text gespeichert in: data/txt/1862_Schulz_J_B/797.txt\n",
      "Text gespeichert in: data/txt/1862_Schulz_J_B/798.txt\n",
      "Text gespeichert in: data/txt/1862_Schulz_J_B/799.txt\n",
      "Text gespeichert in: data/txt/1862_Schulz_J_B/800.txt\n",
      "Text gespeichert in: data/txt/1862_Schulz_J_B/801.txt\n",
      "Text gespeichert in: data/txt/1862_Schulz_J_B/802.txt\n",
      "Text gespeichert in: data/txt/1862_Schulz_J_B/803.txt\n",
      "Text gespeichert in: data/txt/1862_Schulz_J_B/804.txt\n",
      "Text gespeichert in: data/txt/1862_Schulz_J_B/807.txt\n",
      "Text gespeichert in: data/txt/1862_Schulz_J_B/808.txt\n",
      "Text gespeichert in: data/txt/1862_Schulz_J_B/809.txt\n",
      "Text gespeichert in: data/txt/1855_Stein_J_J_F/281.txt\n",
      "Text gespeichert in: data/txt/1855_Stein_J_J_F/282.txt\n",
      "Text gespeichert in: data/txt/1855_Stein_J_J_F/283.txt\n",
      "Text gespeichert in: data/txt/1855_Stein_J_J_F/284.txt\n",
      "Text gespeichert in: data/txt/1855_Stein_J_J_F/285.txt\n",
      "Text gespeichert in: data/txt/1855_Stein_J_J_F/286.txt\n",
      "Text gespeichert in: data/txt/1855_Stein_J_J_F/287.txt\n",
      "Text gespeichert in: data/txt/1855_Stein_J_J_F/288.txt\n",
      "Text gespeichert in: data/txt/1855_Stein_J_J_F/289.txt\n",
      "Text gespeichert in: data/txt/1855_Stein_J_J_F/290.txt\n",
      "Text gespeichert in: data/txt/1855_Stein_J_J_F/291.txt\n",
      "Text gespeichert in: data/txt/1855_Stein_J_J_F/292.txt\n",
      "Text gespeichert in: data/txt/1855_Stein_J_J_F/293.txt\n",
      "Text gespeichert in: data/txt/1855_Stein_J_J_F/294.txt\n",
      "Text gespeichert in: data/txt/1855_Stein_J_J_F/295.txt\n",
      "Text gespeichert in: data/txt/1855_Stein_J_J_F/296.txt\n"
     ]
    }
   ],
   "source": [
    "# Durchlaufe alle Unterordner\n",
    "for folder_name in os.listdir(base_folder):\n",
    "    folder_path = os.path.join(base_folder, folder_name)\n",
    "\n",
    "    # Nur Verzeichnisse berücksichtigen\n",
    "    if not os.path.isdir(folder_path):\n",
    "        continue\n",
    "\n",
    "    # Alle .jpg-Dateien holen\n",
    "    jpeg_files = sorted([f for f in os.listdir(folder_path) if f.lower().endswith('.jpg')])\n",
    "\n",
    "    if not jpeg_files:\n",
    "        print(f\"Keine .jpg-Dateien in Ordner: {folder_name}\")\n",
    "        continue\n",
    "\n",
    "    # Zielunterordner erstellen, falls er nicht existiert\n",
    "    output_folder = os.path.join(output_base, folder_name)\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Bilder verarbeiten\n",
    "    for jpeg_file in jpeg_files:\n",
    "        img_path = os.path.join(folder_path, jpeg_file)\n",
    "        img = cv2.imread(img_path)\n",
    "\n",
    "        # Bildvorverarbeitung\n",
    "        inverted_image = cv2.bitwise_not(img)\n",
    "        gray_image = cv2.cvtColor(inverted_image, cv2.COLOR_BGR2GRAY)\n",
    "        _, binary_image = cv2.threshold(gray_image, 120, 255, cv2.THRESH_BINARY)\n",
    "        binary_image_contrast = cv2.convertScaleAbs(binary_image, alpha=2.0, beta=0)\n",
    "\n",
    "        # OCR\n",
    "        ocr_result = pytesseract.image_to_string(binary_image_contrast, lang=\"deu+frk\")\n",
    "\n",
    "        # TXT-Dateiname & Pfad\n",
    "        txt_filename = os.path.splitext(jpeg_file)[0] + \".txt\"\n",
    "        txt_path = os.path.join(output_folder, txt_filename)\n",
    "\n",
    "        # Speichern\n",
    "        with open(txt_path, \"w\", encoding=\"utf-8\") as txt_file:\n",
    "            txt_file.write(ocr_result)\n",
    "\n",
    "        print(f\"Text gespeichert in: {txt_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER-Erkennung und Erstellung von den XML-Dateien"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grundstruktur von den XML-Dateien "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from xml.dom import minidom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zugang zu den TXT-Dateien wird definiert\n",
    "base_folder = \"data/txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Textbereinigung\n",
    "def clean_text(text):\n",
    "    if not text:\n",
    "        return ''\n",
    "    text = text.replace('\\n', ' ').replace('\\t', ' ')\n",
    "    text = re.sub(r'(?<=[A-Za-z])<', 'ch', text)\n",
    "    text = re.sub(r'(?<=[A-Za-z])>', 'ck', text)\n",
    "    text = (text.replace(\"ic)\", \"ich\")\n",
    "                .replace(\"auc)\", \"auch\")\n",
    "                .replace(\"nac)\", \"nach\")               \n",
    "                .replace(\"aud)\", \"auch\")\n",
    "                .replace(\"Jh\", \"Ich\")\n",
    "                .replace(\"IJ)\", \"Ich\")\n",
    "                .replace(\"nad)\", \"nach\")\n",
    "                .replace(\"» \", \"\")\n",
    "                .replace(\"- \", \"\")\n",
    "                .replace(\" —\", \" \")\n",
    "                .replace(\"| \", \" \")\n",
    "                .replace(\"id)\", \"ich\")\n",
    "                .replace('!', '')\n",
    "                .replace('/', '')\n",
    "                .replace('-', '')\n",
    "                .replace(\"'\", '')\n",
    "                .replace(',', '')\n",
    "                .replace(\"— \", \" \")\n",
    "                .replace(\"„\", \"\")\n",
    "                .replace(\"“\", \"\"))\n",
    "    text = ' '.join(text.split())\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XML-Datei erstellt: data/xml/1851_Kölbing_Friedrich.xml\n",
      "XML-Datei erstellt: data/xml/1860_Suhl_D_W.xml\n",
      "XML-Datei erstellt: data/xml/1847_Schmitt_J_H.xml\n",
      "XML-Datei erstellt: data/xml/1856_Lemmerz_J.xml\n",
      "XML-Datei erstellt: data/xml/1855_Hoffmann_M_E.xml\n",
      "XML-Datei erstellt: data/xml/1893_Bonatz_J_A.xml\n",
      "XML-Datei erstellt: data/xml/1875_Breutel_J_C.xml\n",
      "XML-Datei erstellt: data/xml/1849_Hoffman_Johannes_Friedrich.xml\n",
      "XML-Datei erstellt: data/xml/1820_Schulz_Johann_Gottlieb.xml\n",
      "XML-Datei erstellt: data/xml/1884_Stuhl_S_E.xml\n",
      "XML-Datei erstellt: data/xml/1862_Lemmerz_A.xml\n",
      "XML-Datei erstellt: data/xml/1861_Kölbing_C_R.xml\n",
      "XML-Datei erstellt: data/xml/1862_Beck_J_C.xml\n",
      "XML-Datei erstellt: data/xml/1823_Bonatz_Johanna.xml\n",
      "XML-Datei erstellt: data/xml/1889_Wedemann_J_F.xml\n",
      "XML-Datei erstellt: data/xml/1836_Küster_Johann_Adolph.xml\n",
      "XML-Datei erstellt: data/xml/1830_Bonatz_J_G.xml\n",
      "XML-Datei erstellt: data/xml/1859_Fritsch_Johannes.xml\n",
      "XML-Datei erstellt: data/xml/1866_Nauhaus_Carl.xml\n",
      "XML-Datei erstellt: data/xml/1862_Schulz_J_B.xml\n",
      "XML-Datei erstellt: data/xml/1855_Stein_J_J_F.xml\n"
     ]
    }
   ],
   "source": [
    "# Die Dateien werden iteriert\n",
    "for folder_name in os.listdir(base_folder):\n",
    "    folder_path = os.path.join(base_folder, folder_name)\n",
    "\n",
    "    if not os.path.isdir(folder_path):\n",
    "        continue\n",
    "\n",
    "    txt_files = sorted([f for f in os.listdir(folder_path) if f.lower().endswith('.txt')])\n",
    "\n",
    "    if not txt_files:\n",
    "        print(f\"Keine .txt-Dateien in Ordner: {folder_name}\")\n",
    "        continue\n",
    "    \n",
    "    # === Erstelle die Root-Element der XML-Struktur ===\n",
    "    root = ET.Element(\"TEI\")\n",
    "    root.set(\"version\", \"4.8.1\")\n",
    "    root.set(\"xmlns\", \"http://www.tei-c.org/ns/1.0\")\n",
    "    \n",
    "        # teiHeader-Struktur\n",
    "    headerTEI_el = ET.SubElement(root, \"teiHeader\")\n",
    "    fileDesc_el = ET.SubElement(headerTEI_el, \"fileDesc\")\n",
    "    titleStmt_el = ET.SubElement(fileDesc_el, \"titleStmt\")\n",
    "    \n",
    "    titleStmt_el.append(ET.Comment(\"Titel muss definiert werden. Nach der Korrektur wird der Kommentar gelöscht\"))\n",
    "    ET.SubElement(titleStmt_el, \"title\")\n",
    "\n",
    "    titleStmt_el.append(ET.Comment(\"Autor muss definiert werden. Nach der Korrektur wird der Kommentar gelöscht\"))\n",
    "    ET.SubElement(titleStmt_el, \"author\")\n",
    "\n",
    "    # respStmt\n",
    "    respStmt_el = ET.SubElement(titleStmt_el, \"respStmt\")\n",
    "    resp_el = ET.SubElement(respStmt_el, \"resp\")\n",
    "    resp_el.text = \"XML-Modelling compiled by\"\n",
    "    name_el = ET.SubElement(respStmt_el, \"name\")\n",
    "    name_el.text = \"Svetlana Yakutina\"\n",
    "\n",
    "    # publicationStmt\n",
    "    publicationStmt_el = ET.SubElement(fileDesc_el, \"publicationStmt\")\n",
    "    publisher_el = ET.SubElement(publicationStmt_el, \"publisher\")\n",
    "    orgName_el = ET.SubElement(publisher_el, \"orgName\")\n",
    "    orgName_el.text = \"Verlag der Unitäts-Buchhandlung\"\n",
    "\n",
    "    pubPlace_el = ET.SubElement(publicationStmt_el, \"pubPlace\")\n",
    "    pubPlace_el.text = \"Gnadau (Germany)\"\n",
    "\n",
    "    availability_el = ET.SubElement(publicationStmt_el, \"availability\")\n",
    "    p_header_el = ET.SubElement(availability_el, \"p\")\n",
    "    orgName_el_ = ET.SubElement(p_header_el, \"orgName\")\n",
    "    orgName_el_.text = \"Memorial University of Newfoundland\"\n",
    "\n",
    "    publicationStmt_el.append(ET.Comment(\"Date muss definiert werden. Nach der Korrektur wird der Kommentar gelöscht\"))\n",
    "    ET.SubElement(publicationStmt_el, \"date\")\n",
    "\n",
    "    publicationStmt_el.append(ET.Comment(\"Ref muss definiert werden. Nach der Korrektur wird der Kommentar gelöscht\"))\n",
    "    ET.SubElement(publicationStmt_el, \"ref\")\n",
    "\n",
    "    # sourceDesc Struktur\n",
    "    sourceDesc_el = ET.SubElement(fileDesc_el, \"sourceDesc\")\n",
    "    bibl_el = ET.SubElement(sourceDesc_el, \"bibl\", type=\"j\")\n",
    "    bibl_el.text = \"Nachrichten aus der Brüder-Gemeine\"\n",
    "\n",
    "    # biblFull Struktur\n",
    "    biblFull_el = ET.SubElement(sourceDesc_el, \"biblFull\")\n",
    "    titleStmt_el_ = ET.SubElement(biblFull_el, \"titleStmt\")\n",
    "    ET.SubElement(titleStmt_el_, \"title\").text = \"Nachrichten aus der Brüder-Gemeine\"\n",
    "    ET.SubElement(titleStmt_el_, \"author\").text = \"Unbekannt\"\n",
    "\n",
    "    editionStmt_el = ET.SubElement(biblFull_el, \"editionStmt\")\n",
    "    ET.SubElement(editionStmt_el, \"edition\").text = \"Digitale Ausgabe der Zeitschrift\"\n",
    "\n",
    "    publicationStmt_el_ = ET.SubElement(biblFull_el, \"publicationStmt\")\n",
    "    ET.SubElement(publicationStmt_el_, \"publisher\").text = \"Verlag der Unitäts-Buchhandlung\"\n",
    "\n",
    "    seriesStmt_el = ET.SubElement(biblFull_el, \"seriesStmt\")\n",
    "    title_el__ = ET.SubElement(seriesStmt_el, \"title\", level=\"j\", type=\"main\")\n",
    "    title_el__.text = \"Nachrichten aus der Brüder-Gemeine\"\n",
    "    ET.SubElement(seriesStmt_el, \"biblScope\", unit=\"volume\").text = \"1856-1894\"\n",
    "    seriesStmt_el.append(ET.Comment(\"Erscheinungsjahr muss definiert werden\"))\n",
    "    seriesStmt_el.append(ET.Comment(\"Seiten müssen definiert werden\"))\n",
    "    ET.SubElement(seriesStmt_el, \"biblScope\", unit=\"issue\")\n",
    "    ET.SubElement(seriesStmt_el, \"biblScope\", unit=\"page\")\n",
    "\n",
    "    notesStmt_el = ET.SubElement(biblFull_el, \"notesStmt\")\n",
    "    note_el = ET.SubElement(notesStmt_el, \"note\", type=\"fileFormat\")\n",
    "    note_el.text = \"application/pdf\"\n",
    "\n",
    "    # msDesc Struktur\n",
    "    msDesc_el = ET.SubElement(sourceDesc_el, \"msDesc\")\n",
    "    msIdentifier_el = ET.SubElement(msDesc_el, \"msIdentifier\")\n",
    "    ET.SubElement(msIdentifier_el, \"repository\").text = \"Memorial University of Newfoundland\"\n",
    "    idno_el = ET.SubElement(msIdentifier_el, \"idno\")\n",
    "    idno_el_ = ET.SubElement(idno_el, \"idno\", type=\"URLCatalogue\")\n",
    "    idno_el_.text = \"https://dai.mun.ca/digital/nachrichten/\"\n",
    "\n",
    "    # === Textstruktur ===\n",
    "    text_el = ET.SubElement(root, \"text\")\n",
    "    body_el = ET.SubElement(text_el, \"body\")\n",
    "    div_el = ET.SubElement(body_el, \"div\")\n",
    "\n",
    "    # Durchlaufe alle TXT-Dateien\n",
    "    for txt_file in txt_files:\n",
    "        txt_file_path = os.path.join(folder_path, txt_file)\n",
    "        with open(txt_file_path, 'r', encoding='utf-8') as f:\n",
    "            raw_text = f.read()\n",
    "        clean = clean_text(raw_text)\n",
    "\n",
    "        file_base = os.path.splitext(txt_file)[0]\n",
    "        pb_el = ET.SubElement(div_el, \"pb\", n=file_base)\n",
    "\n",
    "        paragraphs = clean.split('\\n\\n')\n",
    "        for para in paragraphs:\n",
    "            para = para.strip()\n",
    "            if para:\n",
    "                p_el = ET.SubElement(div_el, \"p\")\n",
    "                p_el.text = para\n",
    "                \n",
    "    # XML-Datei erzeugen\n",
    "    xml_str = ET.tostring(root, encoding='utf-8')\n",
    "    pretty_xml = minidom.parseString(xml_str).toprettyxml(indent=\"  \")\n",
    "\n",
    "    os.makedirs(\"data/xml\", exist_ok=True)\n",
    "    output_filename = f\"{folder_name}.xml\"\n",
    "    output_path = os.path.join(\"data/xml\", output_filename)\n",
    "\n",
    "    with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "        output_file.write(pretty_xml)\n",
    "\n",
    "    print(f\"XML-Datei erstellt: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER-Erkennung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import os\n",
    "import re\n",
    "from lxml import etree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modell erfolgreich geladen von spacy_train_v1/output/model-best\n"
     ]
    }
   ],
   "source": [
    "# spaCy-Modell importieren\n",
    "try:\n",
    "    model_path = \"spacy_train_v1/output/model-best\"  # Pfad zum gespeicherten Modell\n",
    "    ner_model = spacy.load(model_path)\n",
    "    print(f\"Modell erfolgreich geladen von {model_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Fehler beim Laden des Modells: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten importieren\n",
    "input_dir = 'data/xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Entferne Punkte direkt nach Zahlen (z.B. \"1.\" → \"1\")\n",
    "    text = re.sub(r'(?<=\\d)\\.(?=\\s|$)', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "def append_text(elem, text):\n",
    "    if elem.text:\n",
    "        elem.text += text\n",
    "    else:\n",
    "        elem.text = text\n",
    "\n",
    "def mark_entities_et(p_elem, ner_model):\n",
    "    if p_elem.text is None and len(p_elem) == 0:\n",
    "        return\n",
    "\n",
    "    # Ursprünglichen Text extrahieren\n",
    "    full_text = \"\".join(p_elem.itertext())\n",
    "    cleaned_text = clean_text(full_text)\n",
    "\n",
    "    # Element leeren\n",
    "    p_elem.clear()\n",
    "\n",
    "    # Sätze splitten\n",
    "    sentences = [s.strip() for s in cleaned_text.split('. ') if s.strip()]\n",
    "\n",
    "    for sentence in sentences:\n",
    "        doc = ner_model(sentence)\n",
    "        ents = [ent for ent in doc.ents if ent.label_ in {\"PER\", \"LOC\", \"DATE\"}]\n",
    "\n",
    "        # Kein NER-Treffer: Satz einfach als Text anhängen\n",
    "        if not ents:\n",
    "            append_text(p_elem, sentence + \". \")\n",
    "            continue\n",
    "\n",
    "        # Satz mit <s>-Tag\n",
    "        s_elem = ET.Element(\"s\")\n",
    "        last_idx = 0\n",
    "\n",
    "        for ent in ents:\n",
    "            # Text vor Entität\n",
    "            if ent.start_char > last_idx:\n",
    "                append_text(s_elem, sentence[last_idx:ent.start_char])\n",
    "\n",
    "            # Entität mit passendem Tag\n",
    "            tag = {\"PER\": \"persName\", \"LOC\": \"placeName\", \"DATE\": \"date\"}[ent.label_]\n",
    "            ent_elem = ET.Element(tag)\n",
    "            ent_elem.text = ent.text\n",
    "            s_elem.append(ent_elem)\n",
    "\n",
    "            last_idx = ent.end_char\n",
    "\n",
    "        # Text nach letzter Entität\n",
    "        if last_idx < len(sentence):\n",
    "            append_text(s_elem, sentence[last_idx:])\n",
    "\n",
    "        # Punkt am Satzende\n",
    "        append_text(s_elem, \". \")\n",
    "\n",
    "        # <s>-Element an Absatz anhängen\n",
    "        p_elem.append(s_elem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_text(parent, text):\n",
    "    \"\"\"Hilfsfunktion: hängt Text an letztes Element oder Parent\"\"\"\n",
    "    if len(parent) > 0:\n",
    "        # Hänge an tail des letzten Elements an\n",
    "        if parent[-1].tail:\n",
    "            parent[-1].tail += text\n",
    "        else:\n",
    "            parent[-1].tail = text\n",
    "    else:\n",
    "        # Hänge direkt an text des Elternelements\n",
    "        if parent.text:\n",
    "            parent.text += text\n",
    "        else:\n",
    "            parent.text = text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XML-Datei wurde aktualisiert: data/xml/1893_Bonatz_J_A.xml\n",
      "XML-Datei wurde aktualisiert: data/xml/1856_Lemmerz_J.xml\n",
      "XML-Datei wurde aktualisiert: data/xml/1820_Schulz_Johann_Gottlieb.xml\n",
      "XML-Datei wurde aktualisiert: data/xml/1836_Küster_Johann_Adolph.xml\n",
      "XML-Datei wurde aktualisiert: data/xml/1847_Schmitt_J_H.xml\n",
      "XML-Datei wurde aktualisiert: data/xml/1859_Fritsch_Johannes.xml\n",
      "XML-Datei wurde aktualisiert: data/xml/1851_Kölbing_Friedrich.xml\n",
      "XML-Datei wurde aktualisiert: data/xml/1866_Nauhaus_Carl.xml\n",
      "XML-Datei wurde aktualisiert: data/xml/1862_Schulz_J_B.xml\n",
      "XML-Datei wurde aktualisiert: data/xml/1855_Hoffmann_M_E.xml\n",
      "XML-Datei wurde aktualisiert: data/xml/1862_Lemmerz_A.xml\n",
      "XML-Datei wurde aktualisiert: data/xml/1889_Wedemann_J_F.xml\n",
      "XML-Datei wurde aktualisiert: data/xml/1830_Bonatz_J_G.xml\n",
      "XML-Datei wurde aktualisiert: data/xml/1861_Kölbing_C_R.xml\n",
      "XML-Datei wurde aktualisiert: data/xml/1884_Stuhl_S_E.xml\n",
      "XML-Datei wurde aktualisiert: data/xml/1875_Breutel_J_C.xml\n",
      "XML-Datei wurde aktualisiert: data/xml/1860_Suhl_D_W.xml\n",
      "XML-Datei wurde aktualisiert: data/xml/1855_Stein_J_J_F.xml\n",
      "XML-Datei wurde aktualisiert: data/xml/1849_Hoffman_Johannes_Friedrich.xml\n",
      "XML-Datei wurde aktualisiert: data/xml/1862_Beck_J_C.xml\n",
      "XML-Datei wurde aktualisiert: data/xml/1823_Bonatz_Johanna.xml\n"
     ]
    }
   ],
   "source": [
    "# Namespace-Map für TEI\n",
    "ns = {'tei': 'http://www.tei-c.org/ns/1.0'}\n",
    "\n",
    "input_dir = 'data/xml'\n",
    "\n",
    "# Durch alle XML-Dateien im Eingabeverzeichnis iterieren\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith(\".xml\"):\n",
    "        file_path = os.path.join(input_dir, filename)\n",
    "\n",
    "        try:\n",
    "            # Lade das XML mit lxml-PARSER\n",
    "            parser = ET.XMLParser(remove_blank_text=True)\n",
    "            tree = ET.parse(file_path, parser)\n",
    "            root = tree.getroot()\n",
    "\n",
    "            # Alle <p>-Elemente finden (inkl. Namespace)\n",
    "            p_elems = root.xpath('.//tei:text//tei:p', namespaces=ns)\n",
    "            for p in p_elems:\n",
    "                mark_entities_et(p, ner_model)  # ← Deine NER-Funktion\n",
    "\n",
    "            # XML-Datei **überschreiben** mit schön formatiertem XML\n",
    "            tree.write(file_path, encoding='utf-8', pretty_print=True, xml_declaration=True)\n",
    "            print(f\"XML-Datei wurde aktualisiert: {file_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Fehler beim Verarbeiten der Datei {filename}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relation-Erkennung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daten vorbereiten "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extraktion der erwähnten Personen-, Organisations- und Ortsnamen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from lxml import etree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Namespace muss außerhalb des Imports definiert werden\n",
    "ns = {'tei': 'http://www.tei-c.org/ns/1.0'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = 'data/xml_kor'\n",
    "output_file = 'data/json/erwähnte_NER.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion zum Säubern der Texte (Zeilenumbrüche, Leerzeichen etc.)\n",
    "def clean_text(text):\n",
    "    if not text:\n",
    "        return ''\n",
    "    text = re.sub(r'\\s+', ' ', text)  # mehrere Whitespaces durch ein Leerzeichen ersetzen\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ergebnisse wurden in data/json/erwähnte_NER.json gespeichert.\n"
     ]
    }
   ],
   "source": [
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith(\".xml\"):\n",
    "        file_path = os.path.join(input_dir, filename)\n",
    "\n",
    "        try:\n",
    "            parser = ET.XMLParser(remove_blank_text=True)\n",
    "            tree = ET.parse(file_path, parser)\n",
    "            root = tree.getroot()\n",
    "            \n",
    "            # Autor extrahieren (nur erster Autor oder leere Zeichenkette)\n",
    "            author_element = root.xpath('.//tei:teiHeader//tei:author', namespaces=ns)\n",
    "            autor = ''\n",
    "            if author_element and author_element[0].text:\n",
    "                autor = clean_text(author_element[0].text)\n",
    "\n",
    "            p_elems = root.xpath('.//tei:text//tei:p', namespaces=ns)\n",
    "\n",
    "            personen = set()\n",
    "            orte = set()\n",
    "            organisationen = set()\n",
    "\n",
    "            for p in p_elems:\n",
    "                pers_names = p.xpath('.//tei:persName[not(ancestor::tei:s)]', namespaces=ns)\n",
    "                org_names = p.xpath('.//tei:orgName[not(ancestor::tei:s)]', namespaces=ns)\n",
    "                place_names = p.xpath('.//tei:placeName[not(ancestor::tei:s)]', namespaces=ns)\n",
    "\n",
    "                for pn in pers_names:\n",
    "                    if pn.text:\n",
    "                        personen.add(clean_text(pn.text))\n",
    "\n",
    "                for on in org_names:\n",
    "                    if on.text:\n",
    "                        organisationen.add(clean_text(on.text))\n",
    "\n",
    "                for pln in place_names:\n",
    "                    if pln.text:\n",
    "                        orte.add(clean_text(pln.text))\n",
    "\n",
    "            result = {\n",
    "                \"datei\": filename,\n",
    "                \"autor\": autor,\n",
    "                \"personen\": sorted(list(personen)),\n",
    "                \"orte\": sorted(list(orte)),\n",
    "                \"organisationen\": sorted(list(organisationen))\n",
    "            }\n",
    "\n",
    "            all_results.append(result)\n",
    "\n",
    "        except ET.XMLSyntaxError as e:\n",
    "            print(f\"Fehler beim Parsen von {filename}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Anderer Fehler bei {filename}: {e}\")\n",
    "\n",
    "# Sicherstellen, dass das Ausgabe-Verzeichnis existiert\n",
    "os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "\n",
    "# Schreibe alles in eine JSON-Datei\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Ergebnisse wurden in {output_file} gespeichert.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sätzenextraktion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from lxml import etree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pfade\n",
    "input_dir = 'data/xml_kor'\n",
    "output_file = 'data/json/sätze.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XML-Namespaces\n",
    "ns = {'tei': 'http://www.tei-c.org/ns/1.0'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion zur Textbereinigung\n",
    "def clean_text(text):\n",
    "    if not text:\n",
    "        return ''\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "\n",
    "# Alle XML-Dateien im Verzeichnis durchgehen\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith(\".xml\"):\n",
    "        file_path = os.path.join(input_dir, filename)\n",
    "\n",
    "        try:\n",
    "            parser = ET.XMLParser(remove_blank_text=True)\n",
    "            tree = ET.parse(file_path, parser)\n",
    "            root = tree.getroot()\n",
    "\n",
    "            # Metadaten: Autor und Referenz\n",
    "            author_element = root.xpath('.//tei:teiHeader//tei:author', namespaces=ns)\n",
    "            autor = clean_text(author_element[0].text) if author_element and author_element[0].text else ''\n",
    "\n",
    "            ref_element = root.xpath('.//tei:teiHeader//tei:ref', namespaces=ns)\n",
    "            ref = clean_text(ref_element[0].text) if ref_element and ref_element[0].text else ''\n",
    "\n",
    "            # HEADS verarbeiten\n",
    "            head_elements = root.xpath('.//tei:text//tei:head', namespaces=ns)\n",
    "            for head in head_elements:\n",
    "                head_text = clean_text(''.join(head.itertext()))\n",
    "\n",
    "                head_personen = set()\n",
    "                head_organisationen = set()\n",
    "                head_orte = set()\n",
    "                head_date = set()\n",
    "\n",
    "                for pn in head.xpath('.//tei:persName', namespaces=ns):\n",
    "                    if pn.text:\n",
    "                        head_personen.add(clean_text(pn.text))\n",
    "                for on in head.xpath('.//tei:orgName', namespaces=ns):\n",
    "                    if on.text:\n",
    "                        head_organisationen.add(clean_text(on.text))\n",
    "                for pln in head.xpath('.//tei:placeName', namespaces=ns):\n",
    "                    if pln.text:\n",
    "                        head_orte.add(clean_text(pln.text))\n",
    "                for dt in head.xpath('.//tei:date', namespaces=ns):\n",
    "                    if dt.text:\n",
    "                        head_date.add(clean_text(dt.text))\n",
    "\n",
    "                result = {\n",
    "                    \"text\": head_text,\n",
    "                    \"personen\": sorted(head_personen),\n",
    "                    \"orte\": sorted(head_orte),\n",
    "                    \"organisationen\": sorted(head_organisationen),\n",
    "                    \"date\": sorted(head_date),\n",
    "                    \"datei\": filename,\n",
    "                    \"autor\": autor,\n",
    "                    \"ref\": ref\n",
    "                }\n",
    "                all_results.append(result)\n",
    "\n",
    "            # SÄTZE (s-Tags) verarbeiten\n",
    "            s_elements = root.xpath('.//tei:text//tei:s', namespaces=ns)\n",
    "            for s in s_elements:\n",
    "                s_text = clean_text(''.join(s.itertext()))\n",
    "\n",
    "                s_personen = set()\n",
    "                s_organisationen = set()\n",
    "                s_orte = set()\n",
    "                s_date = set()\n",
    "\n",
    "                for pn in s.xpath('.//tei:persName', namespaces=ns):\n",
    "                    if pn.text:\n",
    "                        s_personen.add(clean_text(pn.text))\n",
    "                for on in s.xpath('.//tei:orgName', namespaces=ns):\n",
    "                    if on.text:\n",
    "                        s_organisationen.add(clean_text(on.text))\n",
    "                for pln in s.xpath('.//tei:placeName', namespaces=ns):\n",
    "                    if pln.text:\n",
    "                        s_orte.add(clean_text(pln.text))\n",
    "                for dt in s.xpath('.//tei:date', namespaces=ns):\n",
    "                    if dt.text:\n",
    "                        s_date.add(clean_text(dt.text))\n",
    "\n",
    "                result = {\n",
    "                    \"text\": s_text,\n",
    "                    \"personen\": sorted(s_personen),\n",
    "                    \"orte\": sorted(s_orte),\n",
    "                    \"organisationen\": sorted(s_organisationen),\n",
    "                    \"date\": sorted(s_date),\n",
    "                    \"datei\": filename,\n",
    "                    \"autor\": autor,\n",
    "                    \"ref\": ref\n",
    "                }\n",
    "                all_results.append(result)\n",
    "\n",
    "        except ET.XMLSyntaxError as e:\n",
    "            print(f\"Fehler beim Parsen von {filename}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Anderer Fehler bei {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ergebnisse wurden in data/json/sätze.json gespeichert.\n"
     ]
    }
   ],
   "source": [
    "# Sicherstellen, dass das Ausgabe-Verzeichnis existiert\n",
    "os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "\n",
    "# Ergebnisse als JSON speichern\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Ergebnisse wurden in {output_file} gespeichert.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relation erkennen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.prompts import PromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.output_parsers import PydanticOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neue Entity- und Relationstypen\n",
    "entity_types = ['person', 'location', 'organization', 'date']\n",
    "\n",
    "relation_types = [\n",
    "    # Biografische Basisdaten\n",
    "    \"geboren_in\", \"geboren_am\",\n",
    "    \"gestorben_in\", \"gestorben_am\",\n",
    "    \"begraben_in\",\n",
    "\n",
    "    # Wohn- und Wirkorte\n",
    "    \"wohnhaft_in\", \"lebte_in\", \"wirkte_in\", \"aufenthalt_in\",\n",
    "\n",
    "    # Bildung und Beruf\n",
    "    \"ausgebildet_in\", \"studierte_an\",\n",
    "    \"lehre_als\", \"lehre_bei\",\n",
    "    \"tätig_als\", \"tätig_bei\",\n",
    "    \"war\", \"beschäftigt_bei\", \"mitglied_von\",\n",
    "    \"war_eingeschult\", \"unterrichtet_in\",\n",
    "\n",
    "    # Familie\n",
    "    \"verheiratet_mit\", \"kind_von\", \"eltern_von\", \"geschwister_von\",\n",
    "\n",
    "    # Religion & Kirche\n",
    "    \"getauft_am\", \"beigetreten_am\", \"konfirmiert_am\", \"hat_heiliges_abendmahl\",\n",
    "\n",
    "    # Interessen & Themen\n",
    "    \"interessiert_sich_für\", \"beschäftigt_sich_mit\", \"forscht_zu\", \"schreibt_über\",\n",
    "\n",
    "    # Reisen & Aufenthalte\n",
    "    \"gereist_nach\", \"besucht\", \"angekommen_in\", \"abgereist_aus\",\n",
    "    \"aufgehalten_in\", \"zurückgekehrt_nach\", \"verließ\", \"war_in\", \"war_am\", \"gefahren_nach\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtractedInfo(BaseModel):\n",
    "    subjekt: str = Field(description=\"extrahierte Subjekt-Entität, z.B. Daniel Wilhelm Suhl, ich, er, wir, meine l. Frau, seliger Bruder\")\n",
    "    subjekt_type: str = Field(description=\"Typ der Subjekt-Entität, z.B. person\")\n",
    "    prädikat: str = Field(description=\"Beziehung zwischen Subjekt und Objekt\")\n",
    "    objekt: str = Field(description=\"extrahierte Objekt-Entität, z.B. Gnadenthal, Aeltesten-Konferenz\")\n",
    "    objekt_type: str = Field(description=\"Typ der Objekt-Entität, z.B. location, organisation\")\n",
    "    zeit: str = Field(default=None, description=\"Datumsangabe zum Ereignis, z.B. 30 April 1858 oder leer, falls nicht genannt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parser mit neuem Modell\n",
    "parser = PydanticOutputParser(pydantic_object=ExtractedInfo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Systemprompt\n",
    "system_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "Du arbeitest mit biographischen, historischen und religiösen Texten der Herrnhuter Brüdergemeine.\n",
    "Deine Aufgabe ist es, Relationen zwischen Personen, Orten, Organisationen und Daten zu extrahieren.\n",
    "\n",
    "Gib ausschließlich eine JSON-Liste mit Objekten zurück. Jedes Objekt enthält:\n",
    "- \"subjekt\", \"subjekt_type\", \"prädikat\", \"objekt\", \"objekt_type\", \"zeit\".\n",
    "\n",
    "Die erlaubten Entitätstypen sind: {entity_types}\n",
    "Die erlaubten Beziehungstypen sind: {relation_types}\n",
    "\n",
    "KEINE Erklärungen oder zusätzlichen Texte!\n",
    "\"\"\",\n",
    "    input_variables=[\"entity_types\", \"relation_types\"]\n",
    ")\n",
    "system_message_prompt = SystemMessagePromptTemplate(prompt=system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Humanprompt\n",
    "human_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "Hier ist ein Satz aus einem historischen Lebenslauf:\n",
    "\"{text}\"\n",
    "\n",
    "Folgende Entitäten sind im Satz enthalten:\n",
    "personen: {personen}\n",
    "orte: {orte}\n",
    "date: {date}\n",
    "organisationen: {organisationen}\n",
    "\n",
    "Nutze nur die angegebenen Entitäten. Finde sinnvolle Relationen zwischen diesen Entitäten, z. B. wann und wo jemand geboren oder gestorben ist, wo jemand lebte oder tätig war.\n",
    "\n",
    "Wenn sinnvoll, gib zusätzlich im Feld \"zeit\" das relevante Datum an.\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\",\n",
    "    input_variables=[\"text\", \"personen\", \"orte\", \"date\", \"organisationen\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "human_message_prompt = HumanMessagePromptTemplate(prompt=human_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chatmodell + Prompt-Kette\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "model = ChatOllama(model=\"llama3\", temperature=0)\n",
    "chain = LLMChain(llm=model, prompt=chat_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lade Eingabedaten\n",
    "with open(\"data/json/sätze.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    input_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraktion abgeschlossen. Ergebnisse gespeichert in 'extrahierte_relationen.json'.\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "# Extraktion starten\n",
    "for entry in input_data:\n",
    "    result = chain.run(\n",
    "        entity_types=entity_types,\n",
    "        relation_types=relation_types,\n",
    "        text=entry[\"text\"],\n",
    "        personen=\", \".join(entry.get(\"personen\", [])),\n",
    "        orte=\", \".join(entry.get(\"orte\", [])),\n",
    "        date=\", \".join(entry.get(\"date\", [])),\n",
    "        organisationen=\", \".join(entry.get(\"organisationen\", []))\n",
    "    )\n",
    "    results.append({\n",
    "        \"input\": entry,\n",
    "        \"extracted_relations\": result\n",
    "    })\n",
    "\n",
    "# Speichere Ergebnisse\n",
    "with open(\"data/json/graph.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Extraktion abgeschlossen. Ergebnisse gespeichert in 'graph.json'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Input-Datei laden\n",
    "with open(\"data/json/graph.json\", 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Ergebnisliste initialisieren\n",
    "output = []\n",
    "\n",
    "for entry in data:\n",
    "    input_data = entry.get(\"input\", {})\n",
    "    relations_str = entry.get(\"extracted_relations\", \"[]\")\n",
    "    \n",
    "    try:\n",
    "        relations = json.loads(relations_str)\n",
    "    except json.JSONDecodeError:\n",
    "        relations = []\n",
    "\n",
    "    transformed = {\n",
    "        \"text\": input_data.get(\"text\", \"\"),\n",
    "        \"ref\": input_data.get(\"ref\", \"\"),\n",
    "        \"datei\": input_data.get(\"datei\", \"\"),\n",
    "        \"autor\": input_data.get(\"autor\", \"\"),\n",
    "        \"graph\": relations  # genau hier: Liste von Relationen\n",
    "    }\n",
    "\n",
    "    output.append(transformed)\n",
    "\n",
    "# Ausgabe-Datei schreiben\n",
    "with open('data/json/graphen.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(output, f, indent=2, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten anreichnern "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/json/graphen.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame strukturieren\n",
    "records = []\n",
    "for entry in data:\n",
    "    for g in entry.get(\"graph\", []):\n",
    "        records.append({\n",
    "            \"text\": entry.get(\"text\", \"\"),\n",
    "            \"ref\": entry.get(\"ref\", \"\"),\n",
    "            \"nbg\": \"\",\n",
    "            \"datei\": entry.get(\"datei\", \"\"),\n",
    "            \"autor\": entry.get(\"autor\", \"\"),\n",
    "            \"subjekt\": g.get(\"subjekt\", \"\"),\n",
    "            \"subjekt_type\": g.get(\"subjekt_type\", \"\"),\n",
    "            \"q_subjekt\": \"\",\n",
    "            \"prädikat\": g.get(\"prädikat\", \"\"),\n",
    "            \"p_wert\": \"\",\n",
    "            \"objekt\": g.get(\"objekt\", \"\"),\n",
    "            \"objekt_type\": g.get(\"objekt_type\", \"\"),\n",
    "            \"q_objekt\":\"\",\n",
    "            \"zeit\": g.get(\"zeit\", \"\")\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(records)\n",
    "df.to_excel(\"data/json/graphen_1.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_excel(\"data/json/graphen_.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q-ID (Entitäten)\n",
    "def get_wikidata_qid(label):\n",
    "    url = \"https://www.wikidata.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"wbsearchentities\",\n",
    "        \"search\": label,\n",
    "        \"language\": \"de\",\n",
    "        \"format\": \"json\",\n",
    "        \"type\": \"item\",\n",
    "        \"limit\": 1\n",
    "    }\n",
    "    resp = requests.get(url, params=params)\n",
    "    data = resp.json()\n",
    "    if data.get(\"search\"):\n",
    "        return data[\"search\"][0][\"id\"]\n",
    "    return \"\"\n",
    "\n",
    "#P-ID (Properties)\n",
    "def get_property_id(label):\n",
    "    url = \"https://www.wikidata.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"wbsearchentities\",\n",
    "        \"search\": label,\n",
    "        \"language\": \"de\",\n",
    "        \"format\": \"json\",\n",
    "        \"type\": \"property\",\n",
    "        \"limit\": 1\n",
    "    }\n",
    "    resp = requests.get(url, params=params)\n",
    "    data = resp.json()\n",
    "    if data.get(\"search\"):\n",
    "        return data[\"search\"][0][\"id\"]\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-ID für Subjekte\n",
    "unique_subjekte = df[\"subjekt\"].unique()\n",
    "qid_map_subjekt = {}\n",
    "for subj in unique_subjekte:\n",
    "    qid = get_wikidata_qid(subj)\n",
    "    qid_map_subjekt[subj] = qid\n",
    "\n",
    "df[\"q_subjekt\"] = df[\"subjekt\"].map(qid_map_subjekt)\n",
    "\n",
    "# Q-ID für Objekte (achte auf den richtigen Spaltennamen: q_objekt)\n",
    "unique_objekte = df[\"objekt\"].unique()\n",
    "qid_map_objekt = {}\n",
    "for obj in unique_objekte:\n",
    "    qid = get_wikidata_qid(obj)\n",
    "    qid_map_objekt[obj] = qid\n",
    "\n",
    "df[\"q_objekt\"] = df[\"objekt\"].map(qid_map_objekt)\n",
    "\n",
    "# P-ID für Prädikate (nutze get_property_id!)\n",
    "unique_prädikate = df[\"prädikat\"].unique()\n",
    "pid_map = {}\n",
    "for prd in unique_prädikate:\n",
    "    pid = get_property_id(prd)\n",
    "    pid_map[prd] = pid\n",
    "\n",
    "df[\"p_wert\"] = df[\"prädikat\"].map(pid_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_excel(\"data/json/graphen_kor.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_person = pd.read_excel(\"data/personen.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weg = pd.read_excel(\"data/weg.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coordinates(qid):\n",
    "    if not qid or not isinstance(qid, str) or not qid.startswith(\"Q\"):\n",
    "        return None, None\n",
    "\n",
    "    url = \"https://query.wikidata.org/sparql\"\n",
    "    query = f\"\"\"\n",
    "    SELECT ?coord WHERE {{\n",
    "      wd:{qid} wdt:P625 ?coord.\n",
    "    }}\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"Accept\": \"application/sparql-results+json\",\n",
    "        \"User-Agent\": \"GeoKoordinatenBot/1.0 (example@domain.com)\"  # Eigene Adresse oder Projektname\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, params={\"query\": query}, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        data = response.json()\n",
    "        bindings = data.get(\"results\", {}).get(\"bindings\", [])\n",
    "        if bindings:\n",
    "            coord_str = bindings[0][\"coord\"][\"value\"]\n",
    "            lon, lat = coord_str.replace(\"Point(\", \"\").replace(\")\", \"\").split()\n",
    "            return float(lat), float(lon)\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler bei {qid}: {e}\")\n",
    "    return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Nur für geografische Entitäten (objekt_type == \"location\")\n",
    "df_weg = df[df[\"objekt_type\"] == \"location\"].copy()\n",
    "\n",
    "# Koordinaten abrufen\n",
    "df_weg[\"lat\"], df_weg[\"lon\"] = zip(*df_weg[\"q_objekt\"].map(get_coordinates))\n",
    "\n",
    "# Ergebnis zurück in Haupt-DataFrame schreiben\n",
    "df.loc[df_weg.index, \"lat\"] = df_weg[\"lat\"]\n",
    "df.loc[df_weg.index, \"lon\"] = df_weg[\"lon\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# Excel-Datei laden\n",
    "df_weg = pd.read_excel(\"data/weg.xlsx\")\n",
    "\n",
    "# Funktion zum Abrufen der Koordinaten aus Wikidata\n",
    "def get_coordinates(qid):\n",
    "    if not qid or not isinstance(qid, str) or not qid.startswith(\"Q\"):\n",
    "        return None, None\n",
    "\n",
    "    url = \"https://query.wikidata.org/sparql\"\n",
    "    query = f\"\"\"\n",
    "    SELECT ?coord WHERE {{\n",
    "      wd:{qid} wdt:P625 ?coord.\n",
    "    }}\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"Accept\": \"application/sparql-results+json\",\n",
    "        \"User-Agent\": \"GeoKoordinatenBot/1.0 (example@domain.com)\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, params={\"query\": query}, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        data = response.json()\n",
    "        bindings = data.get(\"results\", {}).get(\"bindings\", [])\n",
    "        if bindings:\n",
    "            coord_str = bindings[0][\"coord\"][\"value\"]\n",
    "            lon, lat = coord_str.replace(\"Point(\", \"\").replace(\")\", \"\").split()\n",
    "            return float(lat), float(lon)\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler bei {qid}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Nur Zeilen mit objekt_type == \"location\"\n",
    "df_locations = df_weg[df_weg[\"objekt_type\"] == \"location\"].copy()\n",
    "\n",
    "# Koordinaten abrufen\n",
    "koordinaten = df_locations[\"q_objekt\"].map(get_coordinates)\n",
    "df_locations[\"lat\"], df_locations[\"lon\"] = zip(*koordinaten)\n",
    "\n",
    "# Zurückschreiben in df_weg\n",
    "df_weg.loc[df_locations.index, \"lat\"] = df_locations[\"lat\"]\n",
    "df_weg.loc[df_locations.index, \"lon\"] = df_locations[\"lon\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weg.to_excel(\"data/weg_mit_koordinaten.xlsx\", index=False)\n",
    "df_weg.to_json(\"weg.json\", orient=\"records\", indent=2, force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_person = pd.read_excel(\"data/personen.xlsx\")\n",
    "df_person.to_json(\"personen.json\", orient=\"records\", indent=2, force_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dateikonvertierung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 'leaflet_routes.json' wurde erstellt.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "# Eingabedatei laden\n",
    "with open(\"weg.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "# Personenrouten aufbauen (als Liste)\n",
    "personen_routes = {}\n",
    "\n",
    "for eintrag in raw_data:\n",
    "    person = eintrag.get(\"subjekt\")\n",
    "    id_person = eintrag.get(\"subjekt_ref\")\n",
    "    location = eintrag.get(\"objekt\")\n",
    "    location_type = eintrag.get(\"prädikat\")\n",
    "    id_location = eintrag.get(\"objekt_ref\")\n",
    "    lat = eintrag.get(\"lat\")\n",
    "    lng = eintrag.get(\"lon\")\n",
    "    date = eintrag.get(\"zeit\")  # Optional, kein Umwandeln nötig\n",
    "\n",
    "    if not (person and location and lat and lng):\n",
    "        continue  # Unvollständige Einträge überspringen\n",
    "\n",
    "    if person not in personen_routes:\n",
    "        personen_routes[person] = {\n",
    "            \"person\": person,\n",
    "            \"person_id\": id_person,\n",
    "            \"color\": \"\",  # Du kannst später im JS eine Farbe setzen\n",
    "            \"route\": []\n",
    "        }\n",
    "\n",
    "    personen_routes[person][\"route\"].append({\n",
    "        \"location\": location,\n",
    "        \"location_id\": id_location,\n",
    "        \"location_type\": location_type,\n",
    "        \"lat\": lat,\n",
    "        \"lng\": lng,\n",
    "        \"date\": date  # ggf. None oder z. B. \"Mai 1842\"\n",
    "    })\n",
    "\n",
    "# Liste erzeugen und exportieren\n",
    "result = list(personen_routes.values())\n",
    "\n",
    "with open(\"leaflet_routes.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(result, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"✅ 'leaflet_routes.json' wurde erstellt.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDF / OWL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "df_person = pd.read_excel(\"data/person_1.xlsx\")\n",
    "df_person.to_json(\"personen_.json\", orient=\"records\", indent=2, force_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

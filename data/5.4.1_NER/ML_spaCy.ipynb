{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelltraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datenkonvertierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"training_data_fixed.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    training_data = [json.loads(line) for line in f if line.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"de\")\n",
    "\n",
    "db = DocBin()\n",
    "\n",
    "for item in training_data:\n",
    "    text, annotations = item[0], item[1]\n",
    "\n",
    "    doc = nlp.make_doc(text)\n",
    "\n",
    "    ents = []\n",
    "    for start, end, label in annotations:\n",
    "\n",
    "        span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "        if span is None:\n",
    "            \n",
    "            raise ValueError(\n",
    "                f\"Kann Span nicht bilden\")\n",
    "        ents.append(span)\n",
    "\n",
    "\n",
    "    doc.ents = ents\n",
    "    db.add(doc)\n",
    "\n",
    "db.to_disk(\"output/train.spacy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[38;5;1m✘ The provided output file already exists. To force overwriting the\n",
      "config file, set the --force or -F flag.\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Trainiere das Modell mit spaCy\n",
    "!python3.11 -m spacy init config config.cfg --lang de --pipeline ner --optimize efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Saving to output directory: output\u001b[0m\n",
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[38;5;4mℹ To switch to GPU 0, use the option: --gpu-id 0\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
      "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  --------  ------  ------  ------  ------\n",
      "  0       0          0.00     44.07    0.00    0.00    0.00    0.00\n",
      "  0     200        352.97   2081.52   88.30   88.35   88.26    0.88\n",
      "  0     400        159.17    613.60   95.37   95.58   95.16    0.95\n",
      "  1     600        144.33    372.67   97.14   97.79   96.50    0.97\n",
      "  2     800        169.46    306.53   99.18   99.21   99.15    0.99\n",
      "  3    1000        207.16    241.09   99.07   98.98   99.17    0.99\n",
      "  4    1200        176.74    139.77   99.53   99.67   99.39    1.00\n",
      "  6    1400        251.09    132.99   99.79   99.74   99.84    1.00\n",
      "  8    1600        276.97    122.47   99.93   99.90   99.96    1.00\n",
      " 10    1800        161.88     60.82   99.95   99.98   99.92    1.00\n",
      " 13    2000        184.17     51.83   99.95   99.92   99.98    1.00\n",
      " 16    2200        286.72     78.71   99.97   99.98   99.96    1.00\n",
      " 21    2400        327.41     79.65   99.98   99.96  100.00    1.00\n",
      " 25    2600        178.37     39.90   99.99   99.98  100.00    1.00\n",
      " 29    2800        248.12     47.93  100.00  100.00  100.00    1.00\n",
      " 33    3000        171.56     31.11   99.99  100.00   99.98    1.00\n",
      " 38    3200        507.62     75.62   99.90   99.88   99.92    1.00\n",
      " 42    3400        704.85     93.27   99.97   99.96   99.98    1.00\n",
      " 46    3600        386.45     59.67   99.99  100.00   99.98    1.00\n",
      " 50    3800        404.53     56.65   99.99   99.98  100.00    1.00\n",
      " 55    4000        249.39     38.43   99.99   99.98  100.00    1.00\n",
      " 59    4200        414.27     58.32   99.98   99.98   99.98    1.00\n",
      " 63    4400        576.12     66.95   99.99  100.00   99.98    1.00\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "output/model-last\n"
     ]
    }
   ],
   "source": [
    "!python3.11 -m spacy train config.cfg --output output --paths.train output/train.spacy --paths.dev output/train.spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelltest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('DATE', 'LOC', 'PER')\n"
     ]
    }
   ],
   "source": [
    "# spaCy-Modell importieren\n",
    "ner_model = spacy.load(\"output/model-best\")\n",
    "\n",
    "# Liste aller NER-Labels anzeigen\n",
    "labels = ner_model.get_pipe(\"ner\").labels\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test auf Textdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\" In meinem 14ten Jahr wurde ich von einem der obgedachten Prediger confirmirt.\n",
    "Im Jahr 1799 kam ich bei einem Strumpfwirker-Meister in die Lehre.\n",
    "Als ich im Jahr 1802 ausgelernt hatte, beschloss ich, sogleich auf die Wanderschaft zu gehen.\n",
    "In Heidelberg, wo ich nun wieder arbeitete.\n",
    "In Gnadau bekam ich sogleich Arbeit auf meiner Profession.\n",
    "Im März 1807 begab ich mich auf die Reise nach Herrnhut.\n",
    "Am 27sten September desselben Jahres wurde ich in den Brüderbund aufgenommen.\n",
    "Im Januar 1809 wurde mir angezeigt, dass ich Arbeit bekommen könnte.\n",
    "Am 6 April 1815 erging der Ruf des Herrn an mich, Ihm bei der Mission in Südafrika zu dienen.\n",
    "Am 25. Juli wurden wir in der Unitäts-Aeltesten-Konferenz abgefertigt.\n",
    "Wir traten am folgenden Tag die Reise an.\n",
    "Am 12. August langten wir in London an.\n",
    "Am 30. September verließen wir London.\n",
    "Am 24. December langten wir in der Capstadt an.\n",
    "Am Nachmittag des 30. Decembers erreichten wir Grönekloof.\n",
    "Nachdem wir mit der Hottentotten - Gemeine das Neujahrs- und Heidenfest 1816 gefeiert hatten, verließen wir Grönekloof.\n",
    "Langten nach einer fünftägigen Reise in Gnadenthal an, dem nunmehrigen Ort meiner Bestimmung.\n",
    "Am 3. März desselben Jahres wurde mir der Antrag gemacht, mit der ledigen Schwester Agnes Jenke in den Stand der heiligen Ehe zu treten.\n",
    "Am 26. März wurden wir getraut.\n",
    "Am 9. Februar 1817 wurden wir durch die Geburt eines Söhnleins erfreut.\n",
    "Langten wir am 8. Mai in Enon an.\n",
    "Am 20. Januar 1822 wurde mir in einer Versammlung des Hausgemeinleins eine vom Bischof Gottlob Martin Schneider ausgefertigte schriftliche Ordination zu einem Diakonus überreicht.\n",
    "Anfangs Februar 1825 verließen wir Kapstadt.\n",
    "Langten wir am 17. April in London an.\n",
    "Am 20. Mai in Kleinwelke eintrafen.\n",
    "Am 13. Juli traten wir die Rückreise nach Südafrika wieder an.\n",
    "Nach einem 14-tägigen Aufenthalt da selbst begaben wir uns über Neuwied und Zeist nach London und von da nach Bedford.\n",
    "Am 25. Februar 1826 langten wir nach 15 Wochen auf der stürmischen See in Kapstadt an.\n",
    "Wir reisten nun über Grönekloof nach Gnadenthal.\n",
    "Im Februar 1828 reisten wir zuvorderst nach der Kapstadt.\n",
    "Wir verließen im März 1829 Gnadenthal.\n",
    "Unser Weg führte uns zuerst nach Enon.\n",
    "Am 17. August 1830 verließen wir Silo.\n",
    "Nach einer zwölftägigen Reise in Enon an.\n",
    "Als wir im November 1839 in Enon anlangten, sah es da selbst gar traurig aus.\n",
    "Nachdem derselbe in Ebersdorf mit der ledigen Schwester Rosalie Bauer verbunden worden und diese unsere geliebten Kinder sich noch einige Zeit bei uns aufhielten, sah ich dieselben mit dankbar gebeugtem Herzen ihrem hohen Berufe entgegen gehen.\n",
    "Johannes Lemmerz, heimgegangen in Kleinwelke den 6. Mai 1855.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spaCy-Modell importieren\n",
    "ner_model = spacy.load(\"output/model-best\")\n",
    "\n",
    "doc = ner_model(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1799 DATE\n",
      "1802 DATE\n",
      "Heidelberg LOC\n",
      "Gnadau LOC\n",
      "März 1807 DATE\n",
      "Herrnhut LOC\n",
      "Januar 1809 DATE\n",
      "6 April 1815 DATE\n",
      "Ihm PER\n",
      "25. Juli DATE\n",
      "London LOC\n",
      "Gnadenthal LOC\n",
      "Schwester Agnes Jenke PER\n",
      "26. März DATE\n",
      "9. Februar 1817 DATE\n",
      "Enon LOC\n",
      "20. Januar 1822 DATE\n",
      "Anfangs Februar 1825 DATE\n",
      "Kapstadt LOC\n",
      "London LOC\n",
      "Kleinwelke LOC\n",
      "Neuwied LOC\n",
      "London LOC\n",
      "Bedford LOC\n",
      "25. Februar 1826 DATE\n",
      "15 Wochen DATE\n",
      "Kapstadt LOC\n",
      "Gnadenthal LOC\n",
      "Februar 1828 DATE\n",
      "Kapstadt LOC\n",
      "März 1829 DATE\n",
      "Enon LOC\n",
      "17. August 1830 DATE\n",
      "Enon LOC\n",
      "November 1839 DATE\n",
      "Enon LOC\n",
      "Ebersdorf LOC\n",
      "Kleinwelke LOC\n",
      "6. Mai DATE\n"
     ]
    }
   ],
   "source": [
    "# Erkenne und gebe die Entitäten aus\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testdatenkonvertierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../5.4.2_RE/sätze.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#JSON-Datei mit Tesdaten erstellen\n",
    "def to_spacy_jsonl(data):\n",
    "    records = []\n",
    "    for item in data:\n",
    "        text = item[\"text\"]\n",
    "        entities = []\n",
    "        for label, spans in [\n",
    "            (\"PER\", item.get(\"personen\", [])),\n",
    "            (\"LOC\", item.get(\"orte\", [])),\n",
    "            (\"ORG\", item.get(\"organisationen\", [])),\n",
    "            (\"DATE\", item.get(\"date\", [])),\n",
    "        ]:\n",
    "            for span_text in spans:\n",
    "                start = text.find(span_text)\n",
    "                end = start + len(span_text)\n",
    "                if start != -1:\n",
    "                    entities.append([start, end, label])\n",
    "        records.append({\n",
    "            \"text\": text,\n",
    "            \"entities\": entities\n",
    "        })\n",
    "    return records\n",
    "\n",
    "# Konvertiere und speichere als JSONL\n",
    "jsonl_data = to_spacy_jsonl(data)\n",
    "jsonl_path = \"test_data.jsonl\"\n",
    "with open(jsonl_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for record in jsonl_data:\n",
    "        json.dump(record, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_bin = DocBin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_data.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        example = json.loads(line)\n",
    "        doc = nlp.make_doc(example[\"text\"])\n",
    "\n",
    "        valid_ents = []\n",
    "        occupied = set()\n",
    "\n",
    "        for start, end, label in example[\"entities\"]:\n",
    "            if any(i in occupied for i in range(start, end)):\n",
    "                continue  # überspringe überlappende Entitäten\n",
    "            span = doc.char_span(start, end, label=label)\n",
    "            if span:\n",
    "                valid_ents.append(span)\n",
    "                occupied.update(range(start, end))\n",
    "\n",
    "        doc.ents = valid_ents\n",
    "        doc_bin.add(doc)\n",
    "\n",
    "doc_bin.to_disk(\"output/test.spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testausführung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ner-Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[38;5;4mℹ To switch to GPU 0, use the option: --gpu-id 0\u001b[0m\n",
      "\u001b[1m\n",
      "================================== Results ==================================\u001b[0m\n",
      "\n",
      "TOK     100.00\n",
      "NER P   50.68 \n",
      "NER R   38.35 \n",
      "NER F   43.66 \n",
      "SPEED   28549 \n",
      "\n",
      "\u001b[1m\n",
      "=============================== NER (per type) ===============================\u001b[0m\n",
      "\n",
      "           P       R       F\n",
      "LOC    59.50   56.14   57.77\n",
      "DATE   66.67   43.14   52.39\n",
      "PER    16.75   13.32   14.84\n",
      "ORG     0.00    0.00    0.00\n",
      "\n",
      "\u001b[38;5;2m✔ Saved results to output/metrics.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3.11 -m spacy evaluate output/model-best output/test.spacy --output output/metrics.json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### spaCy de_core_news_lg Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[38;5;4mℹ To switch to GPU 0, use the option: --gpu-id 0\u001b[0m\n",
      "\u001b[1m\n",
      "================================== Results ==================================\u001b[0m\n",
      "\n",
      "TOK      100.00\n",
      "TAG      -     \n",
      "POS      -     \n",
      "MORPH    -     \n",
      "LEMMA    -     \n",
      "UAS      -     \n",
      "LAS      -     \n",
      "NER P    37.31 \n",
      "NER R    41.85 \n",
      "NER F    39.45 \n",
      "SENT P   -     \n",
      "SENT R   -     \n",
      "SENT F   -     \n",
      "SPEED    9871  \n",
      "\n",
      "\u001b[1m\n",
      "=============================== NER (per type) ===============================\u001b[0m\n",
      "\n",
      "           P       R       F\n",
      "LOC    61.33   79.03   69.06\n",
      "PER    21.87   35.79   27.15\n",
      "DATE    0.00    0.00    0.00\n",
      "MISC    0.00    0.00    0.00\n",
      "ORG    14.05   10.76   12.19\n",
      "\n",
      "\u001b[38;5;2m✔ Saved results to output/news_metrics.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3.11 -m spacy evaluate de_core_news_lg ./output/test.spacy --output output/news_metrics.json\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

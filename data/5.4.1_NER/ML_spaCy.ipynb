{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelltraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datenkonvertierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"training_data.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    training_data = [json.loads(line) for line in f if line.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"de\")\n",
    "\n",
    "db = DocBin()\n",
    "\n",
    "for item in training_data:\n",
    "    text, annotations = item[0], item[1]\n",
    "\n",
    "    doc = nlp.make_doc(text)\n",
    "\n",
    "    ents = []\n",
    "    for start, end, label in annotations:\n",
    "\n",
    "        span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "        if span is None:\n",
    "            \n",
    "            raise ValueError(\n",
    "                f\"Kann Span nicht bilden\")\n",
    "        ents.append(span)\n",
    "\n",
    "\n",
    "    doc.ents = ents\n",
    "    db.add(doc)\n",
    "\n",
    "db.to_disk(\"output/train.spacy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;3m⚠ To generate a more effective transformer-based config (GPU-only),\n",
      "install the spacy-transformers package and re-run this command. The config\n",
      "generated now does not use transformers.\u001b[0m\n",
      "\u001b[38;5;4mℹ Generated config template specific for your use case\u001b[0m\n",
      "- Language: de\n",
      "- Pipeline: ner\n",
      "- Optimize for: efficiency\n",
      "- Hardware: CPU\n",
      "- Transformer: None\n",
      "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
      "config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "#Trainiere das Modell mit spaCy\n",
    "!python3.11 -m spacy init config config.cfg --lang de --pipeline ner --optimize efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Saving to output directory: output\u001b[0m\n",
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[38;5;4mℹ To switch to GPU 0, use the option: --gpu-id 0\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
      "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  --------  ------  ------  ------  ------\n",
      "  0       0          0.00     55.11    0.00    0.00    0.00    0.00\n",
      "  0     200        113.66   2279.88   86.60   90.67   82.88    0.87\n",
      "  0     400        669.46    924.70   91.51   94.50   88.70    0.92\n",
      "  1     600        183.16    747.65   94.48   95.45   93.53    0.94\n",
      "  2     800        205.83    724.07   96.76   97.25   96.27    0.97\n",
      "  2    1000        223.96    594.70   98.15   98.31   97.99    0.98\n",
      "  4    1200        234.76    466.08   98.07   97.99   98.15    0.98\n",
      "  5    1400        443.32    379.55   98.55   98.50   98.61    0.99\n",
      "  6    1600        450.62    386.03   99.48   99.42   99.55    0.99\n",
      "  8    1800        517.74    307.79   99.69   99.75   99.64    1.00\n",
      " 11    2000        486.23    292.15   99.69   99.62   99.76    1.00\n",
      " 14    2200        554.82    226.78   99.90   99.93   99.87    1.00\n",
      " 17    2400        395.68    135.51   99.94   99.96   99.91    1.00\n",
      " 21    2600        482.15    139.07   99.95   99.91   99.98    1.00\n",
      " 25    2800        454.67    126.01   99.94   99.95   99.93    1.00\n",
      " 28    3000        696.93    146.82  100.00  100.00  100.00    1.00\n",
      " 32    3200        337.85     75.70   99.97   99.96   99.98    1.00\n",
      " 36    3400        625.82    110.73   99.98   99.98   99.98    1.00\n",
      " 39    3600        484.22     91.84   99.97   99.96   99.98    1.00\n",
      " 43    3800        420.19     69.18   99.98  100.00   99.96    1.00\n",
      " 46    4000        633.43     83.65  100.00  100.00  100.00    1.00\n",
      " 50    4200        651.71     90.33  100.00  100.00  100.00    1.00\n",
      " 54    4400       1511.87     91.21   99.95   99.93   99.96    1.00\n",
      " 57    4600       3230.89    206.89   99.91   99.91   99.91    1.00\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "output/model-last\n"
     ]
    }
   ],
   "source": [
    "!python3.11 -m spacy train config.cfg --output output --paths.train output/train.spacy --paths.dev output/train.spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelltest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('DATE', 'LOC', 'ORG', 'PER')\n"
     ]
    }
   ],
   "source": [
    "# spaCy-Modell importieren\n",
    "ner_model = spacy.load(\"output/model-best\")\n",
    "\n",
    "# Liste aller NER-Labels anzeigen\n",
    "labels = ner_model.get_pipe(\"ner\").labels\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test auf Textdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\" In meinem 14ten Jahr wurde ich von einem der obgedachten Prediger confirmirt.\n",
    "Im Jahr 1799 kam ich bei einem Strumpfwirker-Meister in die Lehre.\n",
    "Als ich im Jahr 1802 ausgelernt hatte, beschloss ich, sogleich auf die Wanderschaft zu gehen.\n",
    "In Heidelberg, wo ich nun wieder arbeitete.\n",
    "In Gnadau bekam ich sogleich Arbeit auf meiner Profession.\n",
    "Im März 1807 begab ich mich auf die Reise nach Herrnhut.\n",
    "Am 27sten September desselben Jahres wurde ich in den Brüderbund aufgenommen.\n",
    "Im Januar 1809 wurde mir angezeigt, dass ich Arbeit bekommen könnte.\n",
    "Am 6 April 1815 erging der Ruf des Herrn an mich, Ihm bei der Mission in Südafrika zu dienen.\n",
    "Am 25. Juli wurden wir in der Unitäts-Aeltesten-Konferenz abgefertigt.\n",
    "Wir traten am folgenden Tag die Reise an.\n",
    "Am 12. August langten wir in London an.\n",
    "Am 30. September verließen wir London.\n",
    "Am 24. December langten wir in der Capstadt an.\n",
    "Am Nachmittag des 30. Decembers erreichten wir Grönekloof.\n",
    "Nachdem wir mit der Hottentotten - Gemeine das Neujahrs- und Heidenfest 1816 gefeiert hatten, verließen wir Grönekloof.\n",
    "Langten nach einer fünftägigen Reise in Gnadenthal an, dem nunmehrigen Ort meiner Bestimmung.\n",
    "Am 3. März desselben Jahres wurde mir der Antrag gemacht, mit der ledigen Schwester Agnes Jenke in den Stand der heiligen Ehe zu treten.\n",
    "Am 26. März wurden wir getraut.\n",
    "Am 9. Februar 1817 wurden wir durch die Geburt eines Söhnleins erfreut.\n",
    "Langten wir am 8. Mai in Enon an.\n",
    "Am 20. Januar 1822 wurde mir in einer Versammlung des Hausgemeinleins eine vom Bischof Gottlob Martin Schneider ausgefertigte schriftliche Ordination zu einem Diakonus überreicht.\n",
    "Anfangs Februar 1825 verließen wir Kapstadt.\n",
    "Langten wir am 17. April in London an.\n",
    "Am 20. Mai in Kleinwelke eintrafen.\n",
    "Am 13. Juli traten wir die Rückreise nach Südafrika wieder an.\n",
    "Nach einem 14-tägigen Aufenthalt da selbst begaben wir uns über Neuwied und Zeist nach London und von da nach Bedford.\n",
    "Am 25. Februar 1826 langten wir nach 15 Wochen auf der stürmischen See in Kapstadt an.\n",
    "Wir reisten nun über Grönekloof nach Gnadenthal.\n",
    "Im Februar 1828 reisten wir zuvorderst nach der Kapstadt.\n",
    "Wir verließen im März 1829 Gnadenthal.\n",
    "Unser Weg führte uns zuerst nach Enon.\n",
    "Am 17. August 1830 verließen wir Silo.\n",
    "Nach einer zwölftägigen Reise in Enon an.\n",
    "Als wir im November 1839 in Enon anlangten, sah es da selbst gar traurig aus.\n",
    "Nachdem derselbe in Ebersdorf mit der ledigen Schwester Rosalie Bauer verbunden worden und diese unsere geliebten Kinder sich noch einige Zeit bei uns aufhielten, sah ich dieselben mit dankbar gebeugtem Herzen ihrem hohen Berufe entgegen gehen.\n",
    "Johannes Lemmerz, heimgegangen in Kleinwelke den 6. Mai 1855.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spaCy-Modell importieren\n",
    "ner_model = spacy.load(\"output/model-best\")\n",
    "\n",
    "doc = ner_model(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14ten Jahr DATE\n",
      "Jahr 1799 DATE\n",
      "1802 DATE\n",
      "Heidelberg LOC\n",
      "Gnadau LOC\n",
      "März 1807 DATE\n",
      "Herrnhut LOC\n",
      "27sten September DATE\n",
      "Brüderbund ORG\n",
      "Januar 1809 DATE\n",
      "6 April 1815 DATE\n",
      "25. Juli DATE\n",
      "folgenden Tag DATE\n",
      "12. August DATE\n",
      "London LOC\n",
      "30. September DATE\n",
      "London LOC\n",
      "24. December DATE\n",
      "30. Decembers DATE\n",
      "Grönekloof LOC\n",
      "1816 DATE\n",
      "Grönekloof LOC\n",
      "Gnadenthal LOC\n",
      "3. März DATE\n",
      "9. Februar 1817 DATE\n",
      "am 8. Mai DATE\n",
      "Enon LOC\n",
      "20. Januar 1822 DATE\n",
      "Gottlob Martin Schneider PER\n",
      "Diakonus ORG\n",
      "Anfangs Februar 1825 DATE\n",
      "Kapstadt LOC\n",
      "17. April DATE\n",
      "London LOC\n",
      "20. Mai DATE\n",
      "Kleinwelke LOC\n",
      "Neuwied LOC\n",
      "London LOC\n",
      "nach Bedford LOC\n",
      "25. Februar 1826 DATE\n",
      "Kapstadt LOC\n",
      "über Grönekloof LOC\n",
      "Gnadenthal LOC\n",
      "Februar 1828 DATE\n",
      "März 1829 DATE\n",
      "Enon LOC\n",
      "17. August 1830 DATE\n",
      "Silo LOC\n",
      "Enon LOC\n",
      "November 1839 DATE\n",
      "Enon LOC\n",
      "Ebersdorf LOC\n",
      "Schwester Rosalie Bauer PER\n",
      "Kleinwelke LOC\n",
      "6. Mai DATE\n"
     ]
    }
   ],
   "source": [
    "# Erkenne und gebe die Entitäten aus\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testdatenkonvertierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../5.4.2_RE/sätze.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#JSON-Datei mit Tesdaten erstellen\n",
    "def to_spacy_jsonl(data):\n",
    "    records = []\n",
    "    for item in data:\n",
    "        text = item[\"text\"]\n",
    "        entities = []\n",
    "        for label, spans in [\n",
    "            (\"PER\", item.get(\"personen\", [])),\n",
    "            (\"LOC\", item.get(\"orte\", [])),\n",
    "            (\"ORG\", item.get(\"organisationen\", [])),\n",
    "            (\"DATE\", item.get(\"date\", [])),\n",
    "        ]:\n",
    "            for span_text in spans:\n",
    "                start = text.find(span_text)\n",
    "                end = start + len(span_text)\n",
    "                if start != -1:\n",
    "                    entities.append([start, end, label])\n",
    "        records.append({\n",
    "            \"text\": text,\n",
    "            \"entities\": entities\n",
    "        })\n",
    "    return records\n",
    "\n",
    "# Konvertiere und speichere als JSONL\n",
    "jsonl_data = to_spacy_jsonl(data)\n",
    "jsonl_path = \"test_data.jsonl\"\n",
    "with open(jsonl_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for record in jsonl_data:\n",
    "        json.dump(record, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_bin = DocBin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_data.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        example = json.loads(line)\n",
    "        doc = nlp.make_doc(example[\"text\"])\n",
    "\n",
    "        valid_ents = []\n",
    "        occupied = set()\n",
    "\n",
    "        for start, end, label in example[\"entities\"]:\n",
    "            if any(i in occupied for i in range(start, end)):\n",
    "                continue  # überspringe überlappende Entitäten\n",
    "            span = doc.char_span(start, end, label=label)\n",
    "            if span:\n",
    "                valid_ents.append(span)\n",
    "                occupied.update(range(start, end))\n",
    "\n",
    "        doc.ents = valid_ents\n",
    "        doc_bin.add(doc)\n",
    "\n",
    "doc_bin.to_disk(\"output/test.spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testausführung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ner-Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[38;5;4mℹ To switch to GPU 0, use the option: --gpu-id 0\u001b[0m\n",
      "\u001b[1m\n",
      "================================== Results ==================================\u001b[0m\n",
      "\n",
      "TOK     100.00\n",
      "NER P   69.93 \n",
      "NER R   59.30 \n",
      "NER F   64.18 \n",
      "SPEED   27251 \n",
      "\n",
      "\u001b[1m\n",
      "=============================== NER (per type) ===============================\u001b[0m\n",
      "\n",
      "           P       R       F\n",
      "DATE   72.97   68.69   70.77\n",
      "LOC    78.00   75.92   76.95\n",
      "PER    47.99   34.79   40.34\n",
      "ORG    53.33   11.68   19.16\n",
      "\n",
      "\u001b[38;5;2m✔ Saved results to output/metrics.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3.11 -m spacy evaluate output/model-best output/test.spacy --output output/metrics.json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### spaCy de_core_news_lg Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[38;5;4mℹ To switch to GPU 0, use the option: --gpu-id 0\u001b[0m\n",
      "\u001b[1m\n",
      "================================== Results ==================================\u001b[0m\n",
      "\n",
      "TOK      100.00\n",
      "TAG      -     \n",
      "POS      -     \n",
      "MORPH    -     \n",
      "LEMMA    -     \n",
      "UAS      -     \n",
      "LAS      -     \n",
      "NER P    38.18 \n",
      "NER R    41.72 \n",
      "NER F    39.87 \n",
      "SENT P   -     \n",
      "SENT R   -     \n",
      "SENT F   -     \n",
      "SPEED    9644  \n",
      "\n",
      "\u001b[1m\n",
      "=============================== NER (per type) ===============================\u001b[0m\n",
      "\n",
      "           P       R       F\n",
      "LOC    60.41   78.87   68.42\n",
      "PER    25.39   39.17   30.81\n",
      "DATE    0.00    0.00    0.00\n",
      "MISC    0.00    0.00    0.00\n",
      "ORG    16.50   12.41   14.17\n",
      "\n",
      "\u001b[38;5;2m✔ Saved results to output/news_metrics.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3.11 -m spacy evaluate de_core_news_lg ./output/test.spacy --output output/news_metrics.json\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
